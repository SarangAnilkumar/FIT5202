{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3BLWZat2yMC"
   },
   "source": [
    "# FIT5202 Data processing for big data\n",
    "\n",
    "##  Activity: Machine Learning with Spark (Transformer, Estimator and Pipeline API)\n",
    "\n",
    "This week we are going to learn about machine learning with Apache Spark. **``MLlib``** is Apache Spark's scalable machine learning library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "- ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- Persistence: saving and load algorithms, models, and Pipelines\n",
    "- Utilities: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "This week we are going to learn about transformers, estimators and machine learning pipeline in this tutorial activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mIybbka7myP"
   },
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Initialize SparkSession](#one)\n",
    "* [Problem Statement](#problem-statement)\n",
    "* [Data Loading and Exploration](#data-loading)\n",
    "* [Transformers and Estimators](#estimators)\n",
    "    * [StopWordsRemover](#stop-words)\n",
    "    * [StringIndexer](#string-indexer)        \n",
    "    * [OneHotEncoder](#ohe)\n",
    "    * [VectorAssembler](#vector-assembler)\n",
    "    * [ML Algorithm and Prediction](#ml-algorithm)\n",
    "* [Pipeline API](#pipeline)\n",
    "    * [Pipeline API Example](#pipeline-example)\n",
    "* [Testing with real data](#testing)\n",
    "* [Lab Tasks](#lab-task-1)\n",
    "    * [Lab Task 1](#lab-task-1)\n",
    "    * [Lab Task 2](#lab-task-2)\n",
    "    * [Lab Task 3](#lab-task-3)    \n",
    "    * [Lab Task 4](#lab-task-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dY2Y0jAx7wH-"
   },
   "source": [
    "## Initialize Spark Session <a class=\"anchor\" id=\"one\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzGdgbI_3U9o"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>In the cell block below, \n",
    "<ul>\n",
    "    <li>Create a SparkConfig object with application name set as \"Spark ML-Transformer, Estimator and Pipeline\"</li>\n",
    "    <li>specify 2 cores for processing</li>\n",
    "    <li>Use the configuration object to create a spark session named as <strong>spark</strong>.</li>\n",
    "    </ul>\n",
    "    \n",
    "<p><strong style=\"color:red\">Important:</strong> You cannot proceed to other steps without completing this.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import libraries needed from pyspark\n",
    "\n",
    "# TODO: Create Spark Configuration Object\n",
    "\n",
    "# TODO: Create SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement <a class=\"anchor\" id=\"problem-statement\"></a>\n",
    "<hr/>\n",
    "Before we jumpstart coding, it is important to understand the problem and its context. The dataset we are using today is the popular <strong>Adult Income Dataset</strong>.[<a href=\"http://archive.ics.uci.edu/ml/datasets/Adult\" target=\"_BLANK\">Source</a>]\n",
    "The dataset provides different parameters of an individual which might influence his/her income. \n",
    "\n",
    "\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">Objective:</strong> We want to explore and see if different personal attributes of a person influence his/her income and whether we can use these attributes to predict their income levels.</div>\n",
    "\n",
    "### Machine Learning Flow\n",
    "The figure below depicts the flow of the Machine Learning approach we want to take. The <strong>ML Algorithm</strong> we are going to use is <strong>Logistic Regression</strong>. Here, we are not going to get into details of the algorithm. We will look at these in details in coming tutorials.\n",
    "<img src=\"attachment:ML%20Model%20flow.png\" width=\"85%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-processing and Exploration <a class=\"anchor\" name=\"data-loading\"></a>\n",
    "In this step, we shall load the given <code>adult.csv</code> file, examine the data and some basic data cleaning operations like checking for null values and finally select a set of relevant columns for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the CSV File\n",
    "df_adult = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the Schema\n",
    "df_adult.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">IMPORTANT:</strong> The <code>income</code> is our <strong>target variable</strong> also called <strong>label</strong> and we are going to use other <strong>independent</strong> variables to predict the target variable.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the shape of the dataframe\n",
    "print((df_adult.count(), len(df_adult.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics\n",
    "df_adult.describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>\n",
    "For better readability of dataframes, you can also convert the dataframe to a <strong>Pandas</strong> DataFrame. \n",
    "    <span style=\"color:red\">Please note that, we can use this only if we have few rows, since the data is loaded into the driver node.</span>\n",
    "    Try using <code>df_adult.describe().toPandas().head()</code>\n",
    "    \n",
    "<strong style=\"color:#FF5555\">IMPORTANT: </strong>\n",
    "    To use <code>.toPandas</code>, Pandas has to be installed. If not, please use <code>!pip install pandas</code> to install <strong>Pandas</strong> in Jupyter Notebook.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">Exploratory Analysis: </strong>\n",
    "Data Exploration and visualization will be covered in the coming tutorials. Here we will focus on the Featurization part.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we are only considering a set of features from the dataset for our analysis. The columns we want to use are <code>'workclass','education','marital-status','occupation','relationship','race','gender','income'</code>. We are going to create a dataframe with these columns only and use this DataFrame for the rest of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['workclass','education','marital-status','occupation','relationship','race','gender','income']\n",
    "df = df_adult[cols]\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-1\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">1. Lab Task: </strong>\n",
    "    Examine the different unique values the <code>income</code> column has. Display the <code>distinct</code> values of the target variable i.e. the <code>income</code> column. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Missing/Null values\n",
    "Check for missing data, drop the rows for missing data.<a href=\"https://www.datasciencemadesimple.com/count-of-missing-nanna-and-null-values-in-pyspark/\" target=\"_BLANK\">[Read More]</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan(c) | col(c).isNull() | (col(c) == '?'), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8S848bv3ELZ"
   },
   "source": [
    "## Estimators, Transformers and Pipelines <a class=\"anchor\" name=\"estimators\"></a>\n",
    "<hr/>\n",
    "Spark's machine learning library has three main abstractions.\n",
    "<ol>\n",
    "    <li><strong>Transformer:</strong> Takes dataframe as input and returns a new DataFrame with one or more columns appended to it. Implements a <code>.transform()</code> method.</li>\n",
    "    <li><strong>Estimator:</strong> Takes dataframe as input and returns a model. Estimator learns from the data. It implements a <code>.fit()</code> method.</li>\n",
    "    <li><strong>Pipeline:</strong> <strong>Combines</strong> together <code>transformers</code> and <code>estimators</code>. Pipelines implement a <code>.fit</code> method.</li>\n",
    "</ol>\n",
    "\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "    Spark appends columns to pre-existing <strong>immutable</strong> DataFrames rather than performing operations <strong>in-place</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWordsRemover  <a class=\"anchor\" name=\"stop-words\"></a>\n",
    "<code>StopWordsRemover</code> takes input as a sequence of strings and drops all stop words. Stop Words are the words that should be excluded from the input, because they appear frequently and don't carry much meaning.<a href=\"https://spark.apache.org/docs/latest/ml-features.html#stopwordsremover\" target=\"_BLANK\">[Ref]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer  <a class=\"anchor\" name=\"string-indexer\"></a>\n",
    "<code>StringIndexer</code> encodes string columns as indices. It assigns a unique value to each category. We need to define the input column/columns name and the output column/columns name in which we want the results.<a href=\"https://spark.apache.org/docs/latest/ml-features.html#stringindexer\" target=\"_BLANK\">[Read More]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azwrtOXd4V_T"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df_ref = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed_transformer = indexer.fit(df_ref)\n",
    "indexed = indexed_transformer.transform(df_ref)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding Python List Comprehensions\n",
    "The examples below use List Comprehensions in Python. You may frequently see this being used. You can read more about it <a href=\"https://www.programiz.com/python-programming/list-comprehension\" target=\"_BLANK\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting column names from the dataframe \n",
    "inputCols=[x for x in df.columns]\n",
    "print(inputCols)\n",
    "\n",
    "# Note that we have used Python List Comprehension in the above example\n",
    "#This is equivalent to doing \n",
    "inputCols=[]\n",
    "for x in df.columns:\n",
    "    inputCols.append(x)\n",
    "print(inputCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-2\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">2. Lab Task: </strong> Use <code>StringIndexer</code> to encode all the columns from the DataFrame <code>df</code> we created in the previous step. <em>Since all the columns we have are categorical in nature, we want to use <code>StringIndexer</code> to transform them into numerical values.</em>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "    You can pass multiple columns as input and output in <code>StringIndexer</code> using <code>StringIndexer(inputCols=[\"col1\",\"col2\"], outputCols=[\"col1Index\",\"col2Index\"])</code>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the input columns\n",
    "inputCols=[x for x in df.columns]\n",
    "#Define the output columns \n",
    "outputCols=[f'{x}_index' for x in df.columns]\n",
    "# TODO: Initialize StringIndexer (use inputCols and outputCols)\n",
    "indexer = \n",
    "\n",
    "#TODO call the fit and transform() method to get the encoded results \n",
    "df_indexed = \n",
    "\n",
    "#TODO Display the output, only the output columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder (OHE) <a class=\"anchor\" name=\"ohe\"></a>\n",
    "One hot encoding is representation of categorical variables as binary vectors. It works in 2 steps:\n",
    "1. The categorical variables are mapped as integer values\n",
    "2. Each integer value is represented as binary vector\n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/latest/ml-features.html#onehotencoder\" target=\"_BLANK\">[Spark References] </a>\n",
    "<a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python\" target=\"_BLANK\"> [Read More]</a>\n",
    "\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">\n",
    "    <strong style=\"color:#006DAE\">NOTE: </strong><code>OneHotEncoder</code> in Spark does not directly encode the categorical variable. We have converted the categorical variable to numerical using <code>StringIndexer</code> in the above step. Now we can implement the <code>OHE</code> to the <em>numerical columns</em> obtained from the step above.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of OHE from Spark Documentation\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df_ref = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df_ref)\n",
    "encoded = model.transform(df_ref)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-3\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">3. Lab Task: </strong>\n",
    "    Apply <code>OneHotEncoder</code> transformation to all numerical columns in the dataframe. We shouldn't be including the <strong>target</strong> column i.e. <code>income</code> anymore here. We just want to include the features.\n",
    "</div><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE THE CODE WHERE NECESSARY\n",
    "#the outputcols of previous step act as input cols for this step\n",
    "inputCols_OHE = #all output columns from StringIndexer exept the Income\n",
    "outputCols_OHE = [f'{x}_vec' for x in inputCols if x!='income']\n",
    "\n",
    "#Define OneHotEncoder with the appropriate columns\n",
    "encoder = \n",
    "# Call fit and transform to get the encoded results\n",
    "df_encoded = \n",
    "#Display the output columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename the target column to label\n",
    "<code>label</code> is popularly used as the name for the target variable. In supervised learning, we have a <strong>labelled</strong> dataset which is why the column name <strong>label</strong> makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded=df_encoded.withColumnRenamed('income_index', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler <a class=\"anchor\" name=\"vector-assembler\"></a>\n",
    "Finally, once we have transformed the data, we want to combine all the features into a single feature column to train the machine learning model. <code>VectorAssembler</code> combines the given list of columns to a <em>single vector</em> column.\n",
    "<a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\" target=\"_BLANK\">[Spark Ref]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-4\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">4. Lab Task: </strong>\n",
    "    Referring to the example above, use <code>VectorAssembler</code> to combine the feature columns from <b>Task 2</b> to a single column named <strong>features</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE THE CODE WHERE NECESSARY\n",
    "inputCols=#the output columns from Task 3 i.e. OHE\n",
    "\n",
    "#Define the assembler with appropriate input and output columns\n",
    "assembler = \n",
    "#use the asseembler transform() to get encoded results\n",
    "df_final = \n",
    "#Display the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Algorithm and Prediction <a class=\"anchor\" name=\"ml-algorithm\"></a>\n",
    "Here we are using Logistic Regression for the classification. We will explore the details about this algorithm in the next tutorials. Please refer to the <a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\" target=\"_BLANK\">[Spark Docs]</a> for further reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into testing and training set 90% into training and 10% for testing\n",
    "train, test = df_final.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing the Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use the model trained with the training data to give predictions for our test data\n",
    "predicted_data = model.transform(test)\n",
    "predicted_data.select('features','label','prediction').filter(predicted_data.label==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gives the accuracy of the model we have built, \n",
    "trainingSummary = model.summary\n",
    "trainingSummary.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">\n",
    "    <strong style=\"color:#006DAE\">NOTE: </strong>What is your interpretation about the accuracy of the model?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZnEiB8H4kjA"
   },
   "source": [
    "## Pipeline API <a class=\"anchor\" name=\"pipeline\"></a>\n",
    "In machine learning, it is common to run a sequence of algorithms to process and learn from data. We have seen in the example above, there is a sequence of steps to be done to prepare the data for training.\n",
    "Such sequence of steps in Spark can be reqpresented by a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. We will try to convert the above sequence of transformers and estimators into a Pipeline.\n",
    "<img src=\"attachment:ML%20Model%20flow.png\" width=\"70%\">\n",
    "In the figure above, we can the steps <code>StringIndexer, OneHotEncoder, VectorAssembler and MLAlgorithm</code> are plugged into the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline API Example <a class=\"anchor\" name=\"pipeline-example\"></a>\n",
    "An example demonstrating the use of <code>Pipeline</code> taken from <a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline\" target=\"_BLANK\">[Spark Docs]</a> is given below.\n",
    "Go through this example to implement your own <code>Pipeline</code> for <strong>Task 5</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCzcgJnY4XQE"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"win million dollar\", 1.0),\n",
    "    (1, \"Office meeting\", 0.0),\n",
    "    (2, \"Do not miss this opportunity\", 1.0),\n",
    "    (3, \"update your password\", 1.0),\n",
    "    (4, \"Assignment submission\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (5, \"get bonus of 200 dollars\"),\n",
    "    (6, \"change your bank password\"),\n",
    "    (7, \"Next meeting is at 5pm\"),\n",
    "    (8, \"Late submission\"),\n",
    "    (9, \"Daily newsletter\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nenOpObk0d4D"
   },
   "source": [
    "### Congratulations on finishing this activity. See you next week."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5202 - Transformer, Estimator and Pipeline.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
