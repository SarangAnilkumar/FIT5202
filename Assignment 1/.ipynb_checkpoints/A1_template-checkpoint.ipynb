{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 2025 S2 Assignment 1 : Analysing Australian Property Market Data\n",
    "\n",
    "## Table of Contents\n",
    "* [Part 1 : Working with RDD](#part-1)  \n",
    "    - [1.1 Data Preparation and Loading](#1.1)  \n",
    "    - [1.2 Data Partitioning in RDD](#1.2)  \n",
    "    - [1.3 Query/Analysis](#1.3)  \n",
    "* [Part 2 : Working with DataFrames](#2-dataframes)  \n",
    "    - [2.1 Data Preparation and Loading](#2-dataframes)  \n",
    "    - [2.2 Query/Analysis](#2.2)  \n",
    "* [Part 3 :  RDDs vs DataFrame vs Spark SQL](#part-3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Feel free to add Code/Markdown cells as you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Working with RDDs (30%) <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Working with RDD\n",
    "In this section, you will need to create RDDs from the given datasets, perform partitioning in these RDDs and use various RDD operations to answer the queries. \n",
    "\n",
    "1.1.1 Data Preparation and Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.\tWrite the code to create a SparkContext object using SparkSession. To create a SparkSession, you first need to build a SparkConf object that contains information about your application. Use Melbourne time as the session timezone. Give your application an appropriate name and run Spark locally with 4 cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Create a SparkConf object\n",
    "conf = SparkConf()\n",
    "conf.setAppName('Analysing Australian Property Market Data')\n",
    "conf.setMaster('local[4]')\n",
    "conf.set('spark.sql.session.timeZone', 'Australia/Melbourne')\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Load the CSV and JSON files into multiple RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main CSV file\n",
    "nsw_property_rdd = sc.textFile('dataset/nsw_property_price.csv')\n",
    "\n",
    "def parse_custom_json(file_content):\n",
    "    \"\"\"\n",
    "    Parses the specific JSON structure where the data is a value in a single-key dictionary.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    try:\n",
    "        data = json.loads(file_content)\n",
    "        # The actual list of records is the first (and only) value in the dictionary\n",
    "        return list(data.values())[0]\n",
    "    except (json.JSONDecodeError, IndexError):\n",
    "        return []\n",
    "\n",
    "# Use wholeTextFiles to read the file content, then map and flatMap to parse and structure the RDD\n",
    "council_rdd = sc.wholeTextFiles('dataset/council.json').flatMap(lambda x: parse_custom_json(x[1]))\n",
    "property_purpose_rdd = sc.wholeTextFiles('dataset/property_purpose.json').flatMap(lambda x: parse_custom_json(x[1]))\n",
    "zoning_rdd = sc.wholeTextFiles('dataset/zoning.json').flatMap(lambda x: parse_custom_json(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 For each RDD, remove the header rows and display the total count and the first 8 records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of nsw_property_rdd: 4854814\n",
      "First 8 records of nsw_property_rdd:\n",
      "4270509,1400000.00,\"8 C NYARI RD, KENTHURST\",\"2156\",house,,\"\",2.044,H,\"2023-12-14\",\"2024-02-14\",V,\"2/1229857\",142,200,9922,53\n",
      "4329326,1105000.00,\"82 CAMARERO ST, BOX HILL\",\"2765\",house,,\"\",300.2,M,\"2024-01-12\",\"2024-02-09\",R,\"1119/1256791\",143,200,7071,41\n",
      "1864112,55000.00,\"321 AUBURN ST, MOREE\",\"2400\",house,,\"\",847.3,M,\"2023-09-15\",\"2024-01-29\",R,\"17/36061\",192,168,7071,40\n",
      "1869899,680000.00,\"207 GWYDIRFIELD RD, MOREE\",\"2400\",house,,SPRINGVALE,2.023,H,\"2024-01-19\",\"2024-02-09\",R,\"6/251911\",193,168,7071,48\n",
      "1867775,220000.00,\"90 MERRIWA ST, BOGGABILLA\",\"2409\",house,,\"\",2023.0,M,\"2023-12-08\",\"2024-02-09\",R,\"1/1/758127\",194,168,7071,52\n",
      "2738374,690000.00,\"10 PETOSTRUM PL, PORT MACQUARIE\",\"2444\",house,,\"\",672.8,M,\"2023-12-14\",\"2024-02-14\",R,\"94/815767\",242,184,7071,40\n",
      "1608665,661000.00,\"71 MULYAN ST, COMO\",\"2226\",house,,\"\",561.7,M,\"2013-03-23\",\"2013-05-09\",\"3\",\"2/11301\",26440,196,4301,2\n",
      "638909,780208.00,\"38 DUFFY AVE, THORNLEIGH\",\"2120\",house,,\"\",3113.2,M,\"2023-06-27\",\"2024-02-09\",V,\"6, 7/533837 3, 4/1047718\",440,147,9922,23\n",
      "\n",
      "Total count of council_rdd: 220\n",
      "First 8 records of council_rdd:\n",
      "{'council_id': 1, 'council_name': '003'}\n",
      "{'council_id': 2, 'council_name': '011'}\n",
      "{'council_id': 3, 'council_name': '013'}\n",
      "{'council_id': 4, 'council_name': '014'}\n",
      "{'council_id': 5, 'council_name': '020'}\n",
      "{'council_id': 6, 'council_name': '021'}\n",
      "{'council_id': 7, 'council_name': '022'}\n",
      "{'council_id': 8, 'council_name': '024'}\n",
      "\n",
      "Total count of property_purpose_rdd: 865\n",
      "First 8 records of property_purpose_rdd:\n",
      "{'purpose_id': 1, 'primary_purpose': ''}\n",
      "{'purpose_id': 16, 'primary_purpose': '0FFICE'}\n",
      "{'purpose_id': 29, 'primary_purpose': '10 FLATS'}\n",
      "{'purpose_id': 31, 'primary_purpose': '10 UNITS'}\n",
      "{'purpose_id': 115, 'primary_purpose': '2'}\n",
      "{'purpose_id': 159, 'primary_purpose': '2 CAR SPACES'}\n",
      "{'purpose_id': 167, 'primary_purpose': '2 FLATS'}\n",
      "{'purpose_id': 190, 'primary_purpose': '2 SHEDS'}\n",
      "\n",
      "Total count of zoning_rdd: 71\n",
      "First 8 records of zoning_rdd:\n",
      "{'zoning_id': 1, 'zoning': ''}\n",
      "{'zoning_id': 2, 'zoning': 'A'}\n",
      "{'zoning_id': 3, 'zoning': 'AGB'}\n",
      "{'zoning_id': 4, 'zoning': 'B'}\n",
      "{'zoning_id': 5, 'zoning': 'B1'}\n",
      "{'zoning_id': 6, 'zoning': 'B2'}\n",
      "{'zoning_id': 7, 'zoning': 'B3'}\n",
      "{'zoning_id': 8, 'zoning': 'B4'}\n"
     ]
    }
   ],
   "source": [
    "# Process nsw_property_rdd\n",
    "header_nsw = nsw_property_rdd.first()\n",
    "nsw_property_rdd_no_header = nsw_property_rdd.filter(lambda row: row != header_nsw)\n",
    "print(f\"Total count of nsw_property_rdd: {nsw_property_rdd_no_header.count()}\")\n",
    "print(\"First 8 records of nsw_property_rdd:\")\n",
    "for record in nsw_property_rdd_no_header.take(8):\n",
    "    print(record)\n",
    "\n",
    "# Process council_rdd\n",
    "print(f\"\\nTotal count of council_rdd: {council_rdd.count()}\")\n",
    "print(\"First 8 records of council_rdd:\")\n",
    "for record in council_rdd.take(8):\n",
    "    print(record)\n",
    "\n",
    "# Process property_purpose_rdd\n",
    "print(f\"\\nTotal count of property_purpose_rdd: {property_purpose_rdd.count()}\")\n",
    "print(\"First 8 records of property_purpose_rdd:\")\n",
    "for record in property_purpose_rdd.take(8):\n",
    "    print(record)\n",
    "\n",
    "# Process zoning_rdd\n",
    "print(f\"\\nTotal count of zoning_rdd: {zoning_rdd.count()}\")\n",
    "print(\"First 8 records of zoning_rdd:\")\n",
    "for record in zoning_rdd.take(8):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.4 Drop records with invalid information: purpose_id or council_id is null, empty, or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total count of nsw_property_rdd after cleaning: 4836784\n",
      "First 8 records of cleaned nsw_property_rdd:\n",
      "['4270509', '1400000.00', '8 C NYARI RD, KENTHURST', '2156', 'house', '', '', '2.044', 'H', '2023-12-14', '2024-02-14', 'V', '2/1229857', '142', '200', '9922', '53']\n",
      "['4329326', '1105000.00', '82 CAMARERO ST, BOX HILL', '2765', 'house', '', '', '300.2', 'M', '2024-01-12', '2024-02-09', 'R', '1119/1256791', '143', '200', '7071', '41']\n",
      "['1864112', '55000.00', '321 AUBURN ST, MOREE', '2400', 'house', '', '', '847.3', 'M', '2023-09-15', '2024-01-29', 'R', '17/36061', '192', '168', '7071', '40']\n",
      "['1869899', '680000.00', '207 GWYDIRFIELD RD, MOREE', '2400', 'house', '', 'SPRINGVALE', '2.023', 'H', '2024-01-19', '2024-02-09', 'R', '6/251911', '193', '168', '7071', '48']\n",
      "['1867775', '220000.00', '90 MERRIWA ST, BOGGABILLA', '2409', 'house', '', '', '2023.0', 'M', '2023-12-08', '2024-02-09', 'R', '1/1/758127', '194', '168', '7071', '52']\n",
      "['2738374', '690000.00', '10 PETOSTRUM PL, PORT MACQUARIE', '2444', 'house', '', '', '672.8', 'M', '2023-12-14', '2024-02-14', 'R', '94/815767', '242', '184', '7071', '40']\n",
      "['1608665', '661000.00', '71 MULYAN ST, COMO', '2226', 'house', '', '', '561.7', 'M', '2013-03-23', '2013-05-09', '3', '2/11301', '26440', '196', '4301', '2']\n",
      "['638909', '780208.00', '38 DUFFY AVE, THORNLEIGH', '2120', 'house', '', '', '3113.2', 'M', '2023-06-27', '2024-02-09', 'V', '6, 7/533837 3, 4/1047718', '440', '147', '9922', '23']\n"
     ]
    }
   ],
   "source": [
    "def parse_csv_line(line):\n",
    "    return next(csv.reader([line]))\n",
    "\n",
    "nsw_property_rdd_parsed = nsw_property_rdd_no_header.map(parse_csv_line)\n",
    "\n",
    "def is_valid(record):\n",
    "    try:\n",
    "        purpose_id = record[16]\n",
    "        council_id = record[15]\n",
    "        return all([purpose_id, council_id, purpose_id != '0', council_id != '0'])\n",
    "    except IndexError:\n",
    "        return False\n",
    "\n",
    "nsw_property_rdd_cleaned = nsw_property_rdd_parsed.filter(is_valid)\n",
    "print(f\"\\nTotal count of nsw_property_rdd after cleaning: {nsw_property_rdd_cleaned.count()}\")\n",
    "print(\"First 8 records of cleaned nsw_property_rdd:\")\n",
    "for record in nsw_property_rdd_cleaned.take(8):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Partitioning in RDD <a class=\"anchor\" name=\"1.2\"></a>\n",
    "1.2.1 For each RDD, using Spark’s default partitioning, print out the total number of partitions and the number of records in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- nsw_property_rdd_cleaned ---\n",
      "Total number of partitions: 19\n",
      "  Partition 0: 257607 records\n",
      "  Partition 1: 256570 records\n",
      "  Partition 2: 254864 records\n",
      "  Partition 3: 255394 records\n",
      "  Partition 4: 255727 records\n",
      "  Partition 5: 258108 records\n",
      "  Partition 6: 258790 records\n",
      "  Partition 7: 257272 records\n",
      "  Partition 8: 255254 records\n",
      "  Partition 9: 254686 records\n",
      "  Partition 10: 254416 records\n",
      "  Partition 11: 253181 records\n",
      "  Partition 12: 253382 records\n",
      "  Partition 13: 255223 records\n",
      "  Partition 14: 254220 records\n",
      "  Partition 15: 257933 records\n",
      "  Partition 16: 257370 records\n",
      "  Partition 17: 255693 records\n",
      "  Partition 18: 231094 records\n",
      "--------------------------------\n",
      "\n",
      "--- council_rdd ---\n",
      "Total number of partitions: 1\n",
      "  Partition 0: 220 records\n",
      "-------------------\n",
      "\n",
      "--- property_purpose_rdd ---\n",
      "Total number of partitions: 1\n",
      "  Partition 0: 865 records\n",
      "----------------------------\n",
      "\n",
      "--- zoning_rdd ---\n",
      "Total number of partitions: 1\n",
      "  Partition 0: 71 records\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of our RDDs and their names for easy iteration\n",
    "rdds = {\n",
    "    \"nsw_property_rdd_cleaned\": nsw_property_rdd_cleaned,\n",
    "    \"council_rdd\": council_rdd,\n",
    "    \"property_purpose_rdd\": property_purpose_rdd,\n",
    "    \"zoning_rdd\": zoning_rdd\n",
    "}\n",
    "\n",
    "# Iterate through each RDD to get partitioning info\n",
    "for name, rdd in rdds.items():\n",
    "    num_partitions = rdd.getNumPartitions()\n",
    "    # Use glom() to create an RDD of lists, where each list is a partition\n",
    "    # Then map len to get the size of each partition\n",
    "    partition_counts = rdd.glom().map(len).collect()\n",
    "    \n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Total number of partitions: {num_partitions}\")\n",
    "    # Enumerate to print the count for each partition number\n",
    "    for i, count in enumerate(partition_counts):\n",
    "        print(f\"  Partition {i}: {count} records\")\n",
    "    print(\"-\" * (len(name) + 8) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 Answer the following questions:   \n",
    "a) How many partitions do the above RDDs have?  \n",
    "b) How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy? Can you explain why it is partitioned in this number?   \n",
    "c) Assuming we are querying the dataset based on <strong> Property Price</strong>, can you think of a better strategy for partitioning the data based on your available hardware resources?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer for a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer for b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer for c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3 Create a user-defined function (UDF) to transform the date strings from ISO format (YYYY-MM-DD) (e.g. 2025-01-01) to Australian format (DD/Mon/YYYY) (e.g. 01/Jan/2025), then call the UDF to transform two date columns (iso_contract_date and iso_settlement_date) to contract_date and settlement_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records with transformed dates:\n",
      "('4270509', '1400000.00', '8 C NYARI RD, KENTHURST', '2156', 'house', '', '', '2.044', 'H', '14/Dec/2023', '14/Feb/2024', 'V', '2/1229857', '142', '200', '9922', '53')\n",
      "('4329326', '1105000.00', '82 CAMARERO ST, BOX HILL', '2765', 'house', '', '', '300.2', 'M', '12/Jan/2024', '09/Feb/2024', 'R', '1119/1256791', '143', '200', '7071', '41')\n",
      "('1864112', '55000.00', '321 AUBURN ST, MOREE', '2400', 'house', '', '', '847.3', 'M', '15/Sep/2023', '29/Jan/2024', 'R', '17/36061', '192', '168', '7071', '40')\n",
      "('1869899', '680000.00', '207 GWYDIRFIELD RD, MOREE', '2400', 'house', '', 'SPRINGVALE', '2.023', 'H', '19/Jan/2024', '09/Feb/2024', 'R', '6/251911', '193', '168', '7071', '48')\n",
      "('1867775', '220000.00', '90 MERRIWA ST, BOGGABILLA', '2409', 'house', '', '', '2023.0', 'M', '08/Dec/2023', '09/Feb/2024', 'R', '1/1/758127', '194', '168', '7071', '52')\n",
      "Total records before transformation: 4836784\n",
      "Total records after transformation: 4836784\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def transform_date_format(record):\n",
    "    \"\"\"\n",
    "    UDF to transform iso_contract_date and iso_settlement_date to Australian format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Date columns are at index 9 and 10\n",
    "        contract_date_iso = record[9]\n",
    "        settlement_date_iso = record[10]\n",
    "        \n",
    "        # Parse the ISO date string (e.g., \"2023-12-14\")\n",
    "        contract_date_obj = datetime.strptime(contract_date_iso, '%Y-%m-%d')\n",
    "        settlement_date_obj = datetime.strptime(settlement_date_iso, '%Y-%m-%d')\n",
    "        \n",
    "        # Format into the desired Australian format (e.g., \"14/Dec/2023\")\n",
    "        contract_date_aus = contract_date_obj.strftime('%d/%b/%Y')\n",
    "        settlement_date_aus = settlement_date_obj.strftime('%d/%b/%Y')\n",
    "        \n",
    "        # Create a new list with the transformed dates\n",
    "        # Note: We are replacing the old dates with the new ones.\n",
    "        # Alternatively, you could append them as new columns.\n",
    "        new_record = list(record) # Make a mutable copy\n",
    "        new_record[9] = contract_date_aus\n",
    "        new_record[10] = settlement_date_aus\n",
    "        \n",
    "        return tuple(new_record) # RDDs work best with immutable tuples\n",
    "        \n",
    "    except (ValueError, IndexError):\n",
    "        # If date parsing fails or index is out of bounds, return the original record\n",
    "        return record\n",
    "\n",
    "# Apply the transformation function to our cleaned RDD\n",
    "nsw_property_rdd_dates_transformed = nsw_property_rdd_cleaned.map(transform_date_format)\n",
    "\n",
    "# Display the first 5 records to verify the transformation\n",
    "print(\"First 5 records with transformed dates:\")\n",
    "for record in nsw_property_rdd_dates_transformed.take(5):\n",
    "    print(record)\n",
    "    \n",
    "# Count before transformation\n",
    "count_before = nsw_property_rdd_cleaned.count()\n",
    "\n",
    "# Count after transformation\n",
    "count_after = nsw_property_rdd_dates_transformed.count()\n",
    "\n",
    "print(f\"Total records before transformation: {count_before}\")\n",
    "print(f\"Total records after transformation: {count_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query/Analysis <a class=\"anchor\" name=\"1.3\"></a>\n",
    "For this part, write relevant RDD operations to answer the following queries.\n",
    "\n",
    "1.3.1 Extract the Month (Jan-Dec) information and print the total number of sales by contract date for each Month. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sales by contract month:\n",
      "  Mar: 461519 sales\n",
      "  May: 450172 sales\n",
      "  Nov: 447462 sales\n",
      "  Oct: 433103 sales\n",
      "  Sep: 423994 sales\n",
      "  Aug: 414212 sales\n",
      "  Jun: 408592 sales\n",
      "  Jul: 405107 sales\n",
      "  Dec: 391570 sales\n",
      "  Feb: 386002 sales\n",
      "  Apr: 382872 sales\n",
      "  Jan: 231686 sales\n"
     ]
    }
   ],
   "source": [
    "# 1.3.1 Extract Month and count sales\n",
    "\n",
    "def safe_month_extractor(record):\n",
    "    \"\"\"\n",
    "    Safely extracts the month from a record.\n",
    "    Returns (Month, 1) on success, or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the record is a list/tuple and has enough elements\n",
    "        if isinstance(record, (list, tuple)) and len(record) > 9:\n",
    "            # Attempt to split the date string. This will fail if the format is not DD/Mon/YYYY\n",
    "            month = record[9].split('/')[1]\n",
    "            return (month, 1)\n",
    "        else:\n",
    "            return None\n",
    "    except IndexError:\n",
    "        # This catches cases where the split does not produce at least 2 elements\n",
    "        return None\n",
    "\n",
    "# --- FIX: Use the safe extractor function and filter out failures ---\n",
    "monthly_sales_rdd = nsw_property_rdd_dates_transformed.map(safe_month_extractor) \\\n",
    "                                                      .filter(lambda x: x is not None)\n",
    "\n",
    "# The rest of the code remains the same\n",
    "sales_by_month = monthly_sales_rdd.reduceByKey(lambda a, b: a + b)\n",
    "sorted_sales = sales_by_month.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"Total number of sales by contract month:\")\n",
    "for month, count in sorted_sales.collect():\n",
    "    print(f\"  {month}: {count} sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 Which 5 councils have the largest number of houses? Show their name and the total number of houses. (Note: Each house may appear multiple times if there are more than one sales, you should only count them once.) (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 councils with the largest number of unique houses:\n",
      "  BLACKTOWN: 11640 houses\n",
      "  CITY OF SYDNEY: 10508 houses\n",
      "  THE HILLS SHIRE: 9186 houses\n",
      "  LIVERPOOL: 6931 houses\n",
      "  SUTHERLAND: 6929 houses\n"
     ]
    }
   ],
   "source": [
    "# 1.3.2 Find the 5 councils with the largest number of houses\n",
    "\n",
    "# Step 1: Prepare the property data RDD with a robust filter\n",
    "property_council_rdd = nsw_property_rdd_cleaned \\\n",
    "    .filter(lambda record: isinstance(record, (list, tuple)) and len(record) > 14) \\\n",
    "    .map(lambda record: (record[1], record[14])) \\\n",
    "    .distinct()\n",
    "\n",
    "# Step 2: Count the number of unique houses per council\n",
    "council_house_counts = property_council_rdd.map(lambda record: (record[1], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 3: Prepare the council data RDD for joining\n",
    "council_lookup_rdd = council_rdd.map(lambda record: (str(record['council_id']), record['council_name']))\n",
    "\n",
    "# Step 4: Join the two RDDs\n",
    "joined_rdd = council_house_counts.map(lambda x: (str(x[0]), x[1])).join(council_lookup_rdd)\n",
    "\n",
    "# Step 5: Format the result and get the top 5\n",
    "formatted_rdd = joined_rdd.map(lambda record: (record[1][1], record[1][0]))\n",
    "top_5_councils = formatted_rdd.takeOrdered(5, key=lambda x: -x[1])\n",
    "\n",
    "# Step 6: Print the final result\n",
    "print(\"\\nTop 5 councils with the largest number of unique houses:\")\n",
    "for council, count in top_5_councils:\n",
    "    print(f\"  {council}: {count} houses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Working with DataFrames (45%) <a class=\"anchor\" name=\"2-dataframes\"></a>\n",
    "In this section, you need to load the given datasets into PySpark DataFrames and use DataFrame functions to answer the queries.\n",
    "### 2.1 Data Preparation and Loading\n",
    "\n",
    "2.1.1. Load the CSV/JSON files into separate dataframes. When you create your dataframes, please refer to the metadata file and think about the appropriate data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "\n",
    "# 2.1.1. Load the CSV/JSON files into separate dataframes.\n",
    "\n",
    "# --- FIX for CSV: Load with options to handle formatting issues ---\n",
    "# We let Spark infer the schema initially and add an escape character option.\n",
    "property_df = spark.read.csv(\n",
    "    'dataset/nsw_property_price.csv',\n",
    "    header=True,\n",
    "    inferSchema=True, # Let Spark figure out the types first\n",
    "    escape='\"' # Helps handle quotes within fields\n",
    ")\n",
    "\n",
    "# --- FIX for JSON: Manually parse the custom structure ---\n",
    "def load_custom_json_to_df(file_path, spark_session):\n",
    "    \"\"\"Reads a custom-formatted JSON file and converts it to a DataFrame.\"\"\"\n",
    "    # Read the whole file as a single text record\n",
    "    json_rdd = spark_session.sparkContext.wholeTextFiles(file_path).map(lambda x: x[1])\n",
    "    # Parse the JSON string to get the list of records (which is the first value in the dict)\n",
    "    data_list = json.loads(json_rdd.first()).values()\n",
    "    # Parallelize the list of records into an RDD\n",
    "    data_rdd = spark_session.sparkContext.parallelize(list(data_list)[0])\n",
    "    # Convert the RDD of dictionaries to a DataFrame\n",
    "    return spark_session.createDataFrame(data_rdd)\n",
    "\n",
    "# Load each JSON file using our custom function\n",
    "council_df = load_custom_json_to_df('dataset/council.json', spark)\n",
    "purpose_df = load_custom_json_to_df('dataset/property_purpose.json', spark)\n",
    "zoning_df = load_custom_json_to_df('dataset/zoning.json', spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Display the schema of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Property DataFrame Schema ---\n",
      "root\n",
      " |-- property_id: integer (nullable = true)\n",
      " |-- purchase_price: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- post_code: integer (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- strata_lot_number: integer (nullable = true)\n",
      " |-- property_name: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- area_type: string (nullable = true)\n",
      " |-- iso_contract_date: date (nullable = true)\n",
      " |-- iso_settlement_date: date (nullable = true)\n",
      " |-- nature_of_property: string (nullable = true)\n",
      " |-- legal_description: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- council_id: integer (nullable = true)\n",
      " |-- purpose_id: integer (nullable = true)\n",
      " |-- zone_id: integer (nullable = true)\n",
      "\n",
      "\n",
      "--- Council DataFrame Schema ---\n",
      "root\n",
      " |-- council_id: long (nullable = true)\n",
      " |-- council_name: string (nullable = true)\n",
      "\n",
      "\n",
      "--- Purpose DataFrame Schema ---\n",
      "root\n",
      " |-- primary_purpose: string (nullable = true)\n",
      " |-- purpose_id: long (nullable = true)\n",
      "\n",
      "\n",
      "--- Zoning DataFrame Schema ---\n",
      "root\n",
      " |-- zoning: string (nullable = true)\n",
      " |-- zoning_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Property DataFrame Schema ---\")\n",
    "property_df.printSchema()\n",
    "print(\"\\n--- Council DataFrame Schema ---\")\n",
    "council_df.printSchema()\n",
    "print(\"\\n--- Purpose DataFrame Schema ---\")\n",
    "purpose_df.printSchema()\n",
    "print(\"\\n--- Zoning DataFrame Schema ---\")\n",
    "zoning_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is large, do you need all columns? How to optimize memory usage? Do you need a customized data partitioning strategy? (Note: Think about those questions but you don’t need to answer these questions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 QueryAnalysis  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "Implement the following queries using dataframes. You need to be able to perform operations like transforming, filtering, sorting, joining and group by using the functions provided by the DataFrame API. For each task, display the first 5 results where no output is specified.\n",
    "\n",
    "2.2.1. The area column has two types: (H, A and M): 1 H is one hectare = 10000 sqm, 1A is one acre = 4000 sqm, 1 M is one sqm. Unify the unit to sqm and create a new column called area_sqm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with unified 'area_sqm' column:\n",
      "+------+---------+--------+\n",
      "|  area|area_type|area_sqm|\n",
      "+------+---------+--------+\n",
      "| 2.044|        H| 20440.0|\n",
      "| 300.2|        M|   300.2|\n",
      "| 847.3|        M|   847.3|\n",
      "| 2.023|        H| 20230.0|\n",
      "|2023.0|        M|  2023.0|\n",
      "+------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unify the area column to sqm\n",
    "# 1 Hectare = 10000 sqm\n",
    "# 1 Acre = 4000 sqm\n",
    "# 1 M = 1 sqm\n",
    "\n",
    "property_df_area_unified = property_df.withColumn(\n",
    "    \"area_sqm\",\n",
    "    F.when(F.col(\"area_type\") == 'H', F.col(\"area\") * 10000)\n",
    "     .when(F.col(\"area_type\") == 'A', F.col(\"area\") * 4000)\n",
    "     .otherwise(F.col(\"area\"))\n",
    ")\n",
    "\n",
    "# Display the first 5 results to verify the new column\n",
    "print(\"\\nDataFrame with unified 'area_sqm' column:\")\n",
    "property_df_area_unified.select(\"area\", \"area_type\", \"area_sqm\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2. <pre>The top five property types are: Residence, Vacant Land, Commercial, Farm and Industrial.\n",
    "However, for historical reason, they may have different strings in the database. Please update the primary_purpose with the following rules:\n",
    "a)\tAny purpose that has “HOME”, “HOUSE”, “UNIT” is classified as “Residence”;\n",
    "b)\t“Warehouse”, “Factory”,  “INDUST” should be changed to “Industrial”;\n",
    "c)\tAnything that contains “FARM”(i.e. FARMING), should be changed to “FARM”;\n",
    "d)\t“Vacant”, “Land” should be “Vacant Land”;\n",
    "e)\tAnything that has “COMM”, “Retail”, “Shop” or “Office” are “Cmmercial”.\n",
    "f)\tAll remaining properties, including null and empty purposes, are classified as “Others”.\n",
    "Show the count of each type in a table.\n",
    "(note: Some properties are multi-purpose, e.g. “House & Farm”, it’s fine to count them multiple times.)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Count of Each Property Type ---\n",
      "+-----------+-------+\n",
      "|   category|  count|\n",
      "+-----------+-------+\n",
      "|     Others|3992886|\n",
      "|Vacant Land| 559409|\n",
      "| Commercial| 136905|\n",
      "|       Farm|  73952|\n",
      "|  Residence|  73009|\n",
      "| Industrial|  37070|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Join the property and purpose DataFrames\n",
    "# We join the DataFrame with unified area to the purpose DataFrame to get the purpose descriptions.\n",
    "# We also handle potential nulls in 'primary_purpose' by filling them with an empty string.\n",
    "full_property_df = property_df_area_unified.join(purpose_df, \"purpose_id\", \"left\") \\\n",
    "                                           .fillna({\"primary_purpose\": \"\"})\n",
    "\n",
    "# Step 2: Apply the classification rules to create an array of categories\n",
    "# We use F.array() to create a list of categories for each property based on the conditions.\n",
    "# F.upper() is used to make the string matching case-insensitive.\n",
    "classified_df = full_property_df.withColumn(\"categories\", F.array(\n",
    "    F.when(\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"HOME\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"HOUSE\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"UNIT\")),\n",
    "        \"Residence\"\n",
    "    ),\n",
    "    F.when(\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"WAREHOUSE\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"FACTORY\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"INDUST\")),\n",
    "        \"Industrial\"\n",
    "    ),\n",
    "    F.when(\n",
    "        F.upper(F.col(\"primary_purpose\")).contains(\"FARM\"),\n",
    "        \"Farm\"\n",
    "    ),\n",
    "    F.when(\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"VACANT\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"LAND\")),\n",
    "        \"Vacant Land\"\n",
    "    ),\n",
    "    F.when(\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"COMM\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"RETAIL\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"SHOP\")) |\n",
    "        (F.upper(F.col(\"primary_purpose\")).contains(\"OFFICE\")),\n",
    "        \"Commercial\"\n",
    "    )\n",
    ")).withColumn(\"categories\", F.expr(\"filter(categories, x -> x is not null)\")) # Remove nulls from the array\n",
    "\n",
    "# Step 3: Handle the 'Others' category\n",
    "# If the 'categories' array is empty, we classify it as 'Others'.\n",
    "final_classified_df = classified_df.withColumn(\n",
    "    \"property_category\",\n",
    "    F.when(F.size(F.col(\"categories\")) == 0, F.array(F.lit(\"Others\")))\n",
    "     .otherwise(F.col(\"categories\"))\n",
    ")\n",
    "\n",
    "# Step 4: Explode the array, group by category, and count\n",
    "# F.explode() creates a new row for each element in the 'property_category' array.\n",
    "# This is how we achieve the multi-counting requirement.\n",
    "category_counts = final_classified_df.withColumn(\"category\", F.explode(\"property_category\")) \\\n",
    "                                     .groupBy(\"category\") \\\n",
    "                                     .count() \\\n",
    "                                     .orderBy(F.desc(\"count\")) # Sort for readability\n",
    "\n",
    "# Step 5: Show the final count table\n",
    "print(\"\\n--- Count of Each Property Type ---\")\n",
    "category_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 Find the top 20 properties that make the largest value gain, show their address, suburb, and value increased. To calculate the value gain, the property must have been sold multiple times, “value increase” can be calculated with the last sold price – first sold price, regardless the transactions in between. Print all 20 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 20 Properties with Largest Value Gain ---\n",
      "+--------------------------------+--------------+\n",
      "|address                         |value_increase|\n",
      "+--------------------------------+--------------+\n",
      "|8 ACACIA CCT, WARRIEWOOD        |8.7489E8      |\n",
      "|38 BARRENJOEY RD, MONA VALE     |5.43102135E8  |\n",
      "|358 ANZAC PDE, KINGSFORD        |5.43102135E8  |\n",
      "|1 FORBES RD, PARKES             |5.43102135E8  |\n",
      "|86 VICTORIA RD, ROZELLE         |5.43102135E8  |\n",
      "|322 CANTERBURY RD, CANTERBURY   |5.4301596E8   |\n",
      "|169 WILLOUGHBY RD, NAREMBURN    |5.4301596E8   |\n",
      "|322 CANTERBURY RD, CANTERBURY   |5.4301596E8   |\n",
      "|169 WILLOUGHBY RD, NAREMBURN    |5.4301596E8   |\n",
      "|1234 PRINCES HWY, ENGADINE      |5.4301596E8   |\n",
      "|327 PRINCES HWY, ST PETERS      |5.4301596E8   |\n",
      "|1234 PRINCES HWY, ENGADINE      |5.4301596E8   |\n",
      "|100 PACIFIC HWY, TUGGERAH       |5.42898135E8  |\n",
      "|3986 PACIFIC HWY, GULMARRAD     |5.42714135E8  |\n",
      "| RIVER ST, MACLEAN              |5.42694135E8  |\n",
      "|136 PACIFIC HWY N, COFFS HARBOUR|5.42644135E8  |\n",
      "|255 STEWART ST, BATHURST        |5.42633135E8  |\n",
      "|8 KOSCIUSKO RD, JINDABYNE       |5.4258277E8   |\n",
      "|366 MAITLAND RD, HEXHAM         |5.42444135E8  |\n",
      "|14 VILLIERS ST, GRAFTON         |5.42364135E8  |\n",
      "+--------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find the first and last sale date for each property\n",
    "# We group by property_id and find the minimum and maximum contract dates.\n",
    "date_extremes_df = property_df.groupBy(\"property_id\") \\\n",
    "    .agg(\n",
    "        F.min(\"iso_contract_date\").alias(\"first_sale_date\"),\n",
    "        F.max(\"iso_contract_date\").alias(\"last_sale_date\")\n",
    "    ) \\\n",
    "    .filter(F.col(\"first_sale_date\") < F.col(\"last_sale_date\")) # Ensure the property was sold at least twice\n",
    "\n",
    "# Step 2: Join back to the original DataFrame to get the FIRST sale price\n",
    "# We join on property_id AND the first_sale_date to find the row with the first sale.\n",
    "first_sale_df = date_extremes_df.join(\n",
    "    property_df,\n",
    "    [\n",
    "        date_extremes_df.property_id == property_df.property_id,\n",
    "        date_extremes_df.first_sale_date == property_df.iso_contract_date\n",
    "    ]\n",
    ").select(\n",
    "    date_extremes_df.property_id,\n",
    "    \"first_sale_date\",\n",
    "    \"last_sale_date\",\n",
    "    F.col(\"purchase_price\").alias(\"first_price\"),\n",
    "    \"address\"\n",
    ")\n",
    "\n",
    "# Step 3: Join back AGAIN to get the LAST sale price\n",
    "# We join the result from Step 2 on property_id AND the last_sale_date.\n",
    "last_sale_df = first_sale_df.join(\n",
    "    property_df,\n",
    "    [\n",
    "        first_sale_df.property_id == property_df.property_id,\n",
    "        first_sale_df.last_sale_date == property_df.iso_contract_date\n",
    "    ]\n",
    ").select(\n",
    "    first_sale_df.property_id,\n",
    "    first_sale_df.address,\n",
    "    \"first_price\",\n",
    "    F.col(\"purchase_price\").alias(\"last_price\")\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the value increase and find the top 20\n",
    "value_gain_df = last_sale_df.withColumn(\"value_increase\", F.col(\"last_price\") - F.col(\"first_price\"))\n",
    "\n",
    "top_20_properties = value_gain_df.orderBy(F.desc(\"value_increase\")) \\\n",
    "                                 .limit(20)\n",
    "\n",
    "# Step 5: Display the results\n",
    "print(\"\\n--- Top 20 Properties with Largest Value Gain ---\")\n",
    "top_20_properties.select(\"address\", \"value_increase\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.4 For each season, plot the median house price trend over the years. Seasons in Australia are defined as: (Spring: Sep-Nov, Summer: Dec-Feb, Autumn: Mar-May, Winter: Jun-Aug). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.5 (Open Question) Explore the dataset freely and plot one diagram of your choice. Which columns (at least 2) are highly correlated to the sales price? Discuss the steps of your exploration and the results. (No word limit, please keep concise.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your dicsussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 RDDs vs DataFrame vs Spark SQL (25%) <a class=\"anchor\" name=\"part-3\"></a>\n",
    "Implement the following complex queries using RDD, DataFrame in SparkSQL separately(choose two). Log the time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference between these 2 approaches of your choice.\n",
    "(notes: You can write a multi-step query or a single complex query, the choice is yours. You can reuse the data frame in Part 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Query:\n",
    "<pre>\n",
    "A property investor wants to understand whether the property price and the settlement date are correlated. Here is the conditions:\n",
    "1)\tThe investor is only interested in the last 2 years of the dataset.\n",
    "2)\tThe investor is looking at houses under $2 million.\n",
    "3)\tPerform a bucketing of the settlement date (settlement – contract date\n",
    "range (15, 30, 45, 60, 90 days).\n",
    "4)\tPerform a bucketing of property prices in $500K(e.g. 0-$500K, $500K-$1M, $1M-$1.5M, $1.5-$2M)\n",
    "5)\tCount the number of transactions in each combination and print the result in the following format\n",
    "(Note: It’s fine to count the same property multiple times in this task, it’s based on sales transactions).\n",
    "(Note: You shall show the full table with 40 rows, 2 years *4 price bucket * 5 settlement bucket; 0 count should be displayed as 0, not omitted.)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\tImplement the above query using two approaches of your choice separately and print the results. (Note: Outputs from both approaches of your choice are required, and the results should be the same.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tWhich one is easier to implement, in your opinion? Log the time taken for each query, and observe the query execution time, among DataFrame and SparkSQL, which is faster and why? Please include proper references. (Maximum 500 words.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some ideas on the comparison\n",
    "\n",
    "Armbrust, M., Huai, Y., Liang, C., Xin, R., & Zaharia, M. (2015). Deep Dive into Spark SQL’s Catalyst Optimizer. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "\n",
    "Damji, J. (2016). A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets. Retrieved September 28, 2017, from https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "Data Flair (2017a). Apache Spark RDD vs DataFrame vs DataSet. Retrieved September 28, 2017, from http://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset\n",
    "\n",
    "Prakash, C. (2016). Apache Spark: RDD vs Dataframe vs Dataset. Retrieved September 28, 2017, from http://why-not-learn-something.blogspot.com.au/2016/07/apache-spark-rdd-vs-dataframe-vs-dataset.html\n",
    "\n",
    "Xin, R., & Rosen, J. (2015). Project Tungsten: Bringing Apache Spark Closer to Bare Metal. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
