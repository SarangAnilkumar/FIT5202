{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcWYqAxbFIee"
   },
   "source": [
    "# FIT5202 Data processing for Big data\n",
    "\n",
    "##  Activity: Parallel Aggregation\n",
    "\n",
    "For this tutorial we will implement different operations and aggregations like distinct, group by and order by on Spark DataFrames. In the second part, you will need to use all these operations to answer the lab tasks.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [SparkContext and SparkSession](#one)\n",
    "* [Parallel Aggregation](#two)\n",
    "    * [Group By](#groupby)        \n",
    "    * [Sort By](#sortby)    \n",
    "    * [Distinct](#distinct)    \n",
    "* [Miscellaneous DataFrame Operations](#misc)\n",
    "    * [Describe a column](#describe_column)\n",
    "    * [Adding/Dropping Columns](#add_drop_column)    \n",
    "    * [PySpark Built-in Functions](#pyspark_functions)       \n",
    "    * [User Defined Functions : UDFs](#udf) \n",
    "* [Lab Tasks](#lab-task-1)\n",
    "    * [Lab Task 1](#lab-task-1)\n",
    "    * [Lab Task 2](#lab-task-2)\n",
    "    * [Lab Task 3](#lab-task-3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iChz1a-tk7aP"
   },
   "source": [
    "<a class=\"anchor\" name=\"one\"></a>\n",
    "## Import Spark classes and create Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>In the cell block below, \n",
    "<ul>\n",
    "    <li>Create a SparkConfig object with application name set as \"Parallel Aggregation\"</li>\n",
    "    <li>specify 2 cores for processing</li>\n",
    "    <li>Use the configuration object to create a spark session named as <strong>spark</strong>.</li>\n",
    "    </ul>\n",
    "    \n",
    "<p><strong style=\"color:red\">Important:</strong> You cannot proceed to other steps without completing this.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsQiS58Ak7aQ"
   },
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Parallel Aggregation\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "783QFsKyk7aV"
   },
   "source": [
    "<a class=\"anchor\" name=\"two\"></a>\n",
    "## Parallel Aggregation\n",
    "\n",
    "Now we will implement basic aggregation functionalities and visualise the parallelism embedded in Spark as well as the execution plan and functions done to perform these kind of queries.\n",
    "\n",
    "In this tutorial, you will use two csv files as datasets which contains information about all the athletes that have participated in the Summer and Winter Olympics (athlete_events.csv) as well as the information of their countries (noc_regions.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PM_c05Ck7aW",
    "outputId": "601ea949-37f3-45a8-f64f-f080af2a90da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### DICTIONARY INFO:\n",
      "Number of partitions: 10\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      " |-- Weight: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- NOC: string (nullable = true)\n",
      " |-- Games: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Medal: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read athlete events data as dataframe\n",
    "df_events = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('athlete_events.csv')\n",
    "\n",
    "# Create Views from Dataframes\n",
    "df_events.createOrReplaceTempView(\"sql_events\")\n",
    "\n",
    "## Verifying the number of partitions for each dataframe\n",
    "## You can explore the data of each csv file with the function printSchema()\n",
    "print(f\"####### DICTIONARY INFO:\")\n",
    "print(f\"Number of partitions: {df_events.rdd.getNumPartitions()}\")\n",
    "df_events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fytB-sHek7ad"
   },
   "source": [
    "### Group By <a class=\"anchor\" name=\"groupby\"></a>\n",
    "This part contains a simple aggregation query. Look into the query plan and level of parallelism in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqM6wRhik7ae"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "#### Aggregate the dataset by 'Year' and count the total number of athletes using Dataframe\n",
    "agg_attribute = 'Year'\n",
    "df_count = df_events.groupby(agg_attribute).agg(F.count(agg_attribute).alias('Total'))\n",
    "\n",
    "#### Aggregate the dataset by 'Year' and count the total number of athletes using SQL\n",
    "sql_count = spark.sql('''\n",
    "  SELECT year,count(*)\n",
    "  FROM sql_events\n",
    "  GROUP BY year\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0lO3gQ-k7ai",
    "outputId": "41efc5f2-6e28-40e2-8388-2d6906756b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year='1956', Total=6434),\n",
       " Row(Year='2016', Total=13688),\n",
       " Row(Year='1936', Total=7401),\n",
       " Row(Year='2012', Total=12920),\n",
       " Row(Year='1972', Total=11959)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "  The same thing can be done using \n",
    "    <code>groupby(agg_attribute).agg({'Year':'count'})</code>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort By <a class=\"anchor\" name=\"sortby\"></a>\n",
    "We can use orderBy operation to sort the dataframe based on some column.\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "    You can specify the sort order using the method <strong>desc()</strong>\n",
    "    <code>orderBy(df_events.Year.desc())</code>    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|Year|                Name|                Team|\n",
      "+----+--------------------+--------------------+\n",
      "|1896| G. Karagiannopoulos|              Greece|\n",
      "|1896|         Khatzidakis|              Greece|\n",
      "|1896|Tilemakhos Karakalos|              Greece|\n",
      "|1896| Gyula Kakas (Kokas)|             Hungary|\n",
      "|1896|        Karakatsanis|              Greece|\n",
      "|1896| Gyula Kakas (Kokas)|             Hungary|\n",
      "|1896|Konstantinos Kara...|              Greece|\n",
      "|1896|   Filippos Karvelas|Ethnikos Gymnasti...|\n",
      "|1896|Alexandros Khalko...|              Greece|\n",
      "|1896|         N. Katravas|              Greece|\n",
      "|1896| Gyula Kakas (Kokas)|             Hungary|\n",
      "|1896|   Frederick Keeping|       Great Britain|\n",
      "|1896| Pantelis Karasevdas|              Greece|\n",
      "|1896|   Frederick Keeping|       Great Britain|\n",
      "|1896| Pantelis Karasevdas|              Greece|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_events.select('Year','Name','Team').orderBy(df_events.Year).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INsF2yY_k7an"
   },
   "source": [
    "### Distinct <a class=\"anchor\" name=\"distinct\"></a>\n",
    "This part contains a simple query to get the distinct values of one of the attributes and then sorting them by the same attribute in ascending order.\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "    We can use <code>.sort()</code> method to do the sorting as well. In the second parameter of the method, we can specify the order of the sorting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUMVzlrkk7ar"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year='1896'),\n",
       " Row(Year='1900'),\n",
       " Row(Year='1904'),\n",
       " Row(Year='1906'),\n",
       " Row(Year='1908'),\n",
       " Row(Year='1912'),\n",
       " Row(Year='1920'),\n",
       " Row(Year='1924'),\n",
       " Row(Year='1928'),\n",
       " Row(Year='1932')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Get the distinct values for 'Year' in the dataset using Dataframe\n",
    "df_distinct_sort = df_events.select('Year').distinct().sort('Year', ascending=True)\n",
    "\n",
    "#### Get the distinct values for 'Year' in the dataset using SQL\n",
    "sql_distinct_sort = spark.sql('''\n",
    "  SELECT distinct Year\n",
    "  FROM sql_events\n",
    "  ORDER BY year\n",
    "''')\n",
    "df_distinct_sort.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gNRFFWwk7au",
    "outputId": "6ef467dd-ee39-40c6-efee-157d48a52a5a"
   },
   "source": [
    "<a class=\"anchor\" id=\"lab-task-1\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">1. Lab Task: </strong>Sort the above dataframe i.e. events by <strong>Year</strong> in descending order.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+\n",
      "|                Name|               Event|Year|\n",
      "+--------------------+--------------------+----+\n",
      "|Alexander Ingvar ...|Rowing Men's Coxl...|2016|\n",
      "|Andrs Byron Silva...|Athletics Men's 4...|2016|\n",
      "|Isaac Phillipjuni...|Athletics Men's 1...|2016|\n",
      "|Andriy Oleksiyovy...|Gymnastics Men's ...|2016|\n",
      "|           aba Silai|Swimming Men's 10...|2016|\n",
      "|Arlenis Sierra Ca...|Cycling Women's R...|2016|\n",
      "|Nicole Sifuentes ...|Athletics Women's...|2016|\n",
      "|Alex William Pomb...|Judo Men's Lightw...|2016|\n",
      "|          Alain Sign| Sailing Men's Skiff|2016|\n",
      "|Altobeli Santos d...|Athletics Men's 3...|2016|\n",
      "+--------------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_events.select('Name','Event','Year').sort('Year', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"misc\"></a>\n",
    "## Miscellaneous Dataframe Operations\n",
    "These are the examples of other dataframe operations which are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing a Column <a class=\"anchor\" name=\"describe_column\"></a>\n",
    "The <code>describe()</code> melthod gives the statistical summary of the column. If the column is not specified, it gives the summary of the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------+------------------+------------------+------------------+-----------+------+-----------+------------------+------+-----------+-----------+--------------------+------+\n",
      "|summary|               ID|                Name|   Sex|               Age|            Height|            Weight|       Team|   NOC|      Games|              Year|Season|       City|      Sport|               Event| Medal|\n",
      "+-------+-----------------+--------------------+------+------------------+------------------+------------------+-----------+------+-----------+------------------+------+-----------+-----------+--------------------+------+\n",
      "|  count|           271116|              271116|271116|            271116|            271116|            271116|     271116|271116|     271116|            271116|271116|     271116|     271116|              271116|271116|\n",
      "|   mean|68248.95439590434|                null|  null|25.556898357297374|175.33896987366376| 70.70239290053351|       null|  null|       null|1978.3784800601957|  null|       null|       null|                null|  null|\n",
      "| stddev|39022.28634475653|                null|  null|  6.39356084703581|10.518462222679219|14.348019999019366|       null|  null|       null|29.877631985613505|  null|       null|       null|                null|  null|\n",
      "|    min|                1|  Gabrielle Marie...|     F|                10|               127|               100|30. Februar|   AFG|1896 Summer|              1896|Summer|Albertville|Aeronautics|Aeronautics Mixed...|Bronze|\n",
      "|    max|            99999|            zzet nce|     M|                NA|                NA|                NA|       rn-2|   ZIM|2016 Summer|              2016|Winter|  Vancouver|  Wrestling|Wrestling Women's...|Silver|\n",
      "+-------+-----------------+--------------------+------+------------------+------------------+------------------+-----------+------+-----------+------------------+------+-----------+-----------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_events.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Dropping a column in dataframe <a class=\"anchor\" name=\"add_drop_column\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Years Ago: double, Name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Here is an example of adding a new column based on the previous column\n",
    "df_events_new = df_events.withColumn('Years Ago',2022-df_events.Year).select('Years Ago','Name')\n",
    "display(df_events_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>\n",
    "    You can use the <code>.drop('column_name')</code> method to drop columns from a dataframe. Try this method and drop the column created above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Name: string, Sex: string, Age: string, Height: string, Weight: string, Team: string, NOC: string, Games: string, Year: string, Season: string, City: string, Sport: string, Event: string, Medal: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_events.drop('Years Ago'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PySpark Functions <a class=\"anchor\" name=\"pyspark_functions\"></a>\n",
    "You can use PySpark built-in functions along with the <code>withColumn()</code> API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Name: string, Sex: string, Age: string, Height: string, Weight: string, Team: string, NOC: string, Games: string, Year: string, Season: string, City: string, Sport: string, Event: string, Medal: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#Changing the datatype \n",
    "#using the display method to see the columns and datatypes of a dataframe\n",
    "display(df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Name: string, Sex: string, Age: int, Height: string, Weight: string, Team: string, NOC: string, Games: string, Year: string, Season: string, City: string, Sport: string, Event: string, Medal: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use CAST to change the datatype of Age Column \n",
    "df_events = df_events.withColumn('Age',F.col('Age').cast(IntegerType()))\n",
    "display(df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Games Year|\n",
      "+----------+\n",
      "|      1992|\n",
      "|      2012|\n",
      "|      1920|\n",
      "|      1900|\n",
      "|      1988|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The folowing example uses another inbuilt function to extract year from the Games column\n",
    "df_events = df_events.withColumn('Games Year',F.split(df_events.Games,' ')[0])\n",
    "df_events.select('Games Year').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame UDFs (User Defined Functions) <a class=\"anchor\" name=\"udf\"></a>\n",
    "Similar to map operation in an RDDs, sometimes we might want to apply a complex operation to the DataFrame, something which is not provided by the DataFrame APIs. In such scenarios, using a Spark UDF could be handy. To use Spark UDFs, we need to use the F.udf to convert a regular function to a Spark UDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|      Games|Game Year|\n",
      "+-----------+---------+\n",
      "|1992 Summer|     1992|\n",
      "|2012 Summer|     2012|\n",
      "|1920 Summer|     1920|\n",
      "|1900 Summer|     1900|\n",
      "|1988 Winter|     1988|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+---------+\n",
      "|      Games|Game_Year|\n",
      "+-----------+---------+\n",
      "|1992 Summer|     1992|\n",
      "|2012 Summer|     2012|\n",
      "|1920 Summer|     1920|\n",
      "|1900 Summer|     1900|\n",
      "|1988 Winter|     1988|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For example, the following function does the same things as the above built-function but this time we are using a udf\n",
    "#1. The function is defined\n",
    "def extract_year(s):\n",
    "    return int(s.split(' ')[0])\n",
    "\n",
    "#2. Calling the UDF with DataFrame\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#First Register the function as UDF\n",
    "extract_year_udf = udf(extract_year,IntegerType())\n",
    "\n",
    "#Call the function\n",
    "df_events.select('Games',extract_year_udf('Games').alias(\"Game Year\")).show(5)\n",
    "\n",
    "#4. Calling with Spark SQL\n",
    "#First Register the function as UDF\n",
    "spark.udf.register('extract_year',extract_year,IntegerType())\n",
    "\n",
    "#Call the function \n",
    "df_events.createOrReplaceTempView('events')\n",
    "df_sql = spark.sql('''select Games, extract_year(Games) as Game_Year from events''')\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FrgOeLck7a9"
   },
   "source": [
    "## Combining DataFrame operations <a class=\"anchor\" name=\"combine\"></a>\n",
    "Now that we have used the main SQL operations to process data, you will implement several queries using Spark Dataframes and SQL to solve each of the queries.\n",
    "The dataset used for this section will be the 2 attached csv files:\n",
    "* <code>athlete_events.csv</code>\n",
    "* <code>noc_regions.csv</code>\n",
    "\n",
    "The first dataset was already used in the first part of this tutorial. The second one contains the countries with some additional information\n",
    "In this section, you will need to complete most of the code but in some parts, a hint or the name of variables will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMgH70wik7a9"
   },
   "outputs": [],
   "source": [
    "# Stop the previous Spark Context to clean all the previous executions from the previous section\n",
    "sc.stop()\n",
    "# Verify that the Spark UI is not running anymore or that there is no content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mh-9XqEIk7bB"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>\n",
    "Since we have removed the Spark Context in the previous code block, start the context once again by using the SparkSession object in the next code block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptcMBponk7bC"
   },
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Parallel Aggregation\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PChZJoTk7bE"
   },
   "source": [
    "### Create Spark data objects (Dataframes and SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7nelKMik7bF",
    "outputId": "0a6028ad-9c4e-4ba1-f3ce-3e67d38b8b77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      " |-- Weight: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- NOC: string (nullable = true)\n",
      " |-- Games: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Medal: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- NOC: string (nullable = true)\n",
      " |-- Population: string (nullable = true)\n",
      " |-- GDP per Capita: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read athlete events data as dataframe\n",
    "df_events = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('athlete_events.csv')\n",
    "\n",
    "# TODO: Read noc regions (countries) data as dataframe\n",
    "df_regions = spark.read.format('csv')\\\n",
    "            .option('header',True)\\\n",
    "            .load('noc_regions.csv')\n",
    "\n",
    "# Create Views from Dataframes\n",
    "df_events.createOrReplaceTempView(\"sql_events\")\n",
    "df_regions.createOrReplaceTempView(\"sql_regions\")\n",
    "\n",
    "# View Schema for both dataframes\n",
    "df_events.printSchema()\n",
    "df_regions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0noHiLVk7bI"
   },
   "source": [
    "### Queries/Anaysis\n",
    "For this part, you will need to implement the Dataframe operations and/or the SQL queries to obtain the reports needed for the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-2\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">2. Lab Task: </strong>Get total number of male athletes per year of the 2000s order by ascending year. <strong>Sample Output:</strong>\n",
    "<pre>\n",
    "+----+------------------+\n",
    "|year|number_of_athletes|\n",
    "+----+------------------+\n",
    "|2000|             XXXXX|\n",
    "|2002|              XXXX|\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year='2000', number_of_male_athletes=8390), Row(year='2002', number_of_male_athletes=2527), Row(year='2004', number_of_male_athletes=7897), Row(year='2006', number_of_male_athletes=2625), Row(year='2008', number_of_male_athletes=7786), Row(year='2010', number_of_male_athletes=2555), Row(year='2012', number_of_male_athletes=7105), Row(year='2014', number_of_male_athletes=2868), Row(year='2016', number_of_male_athletes=7465)]\n"
     ]
    }
   ],
   "source": [
    "## Dataframe Solution\n",
    "df_res = df_events.filter(col('Sex')=='M').filter(col('Year')>=2000)\\\n",
    "            .groupby('Year').agg(F.count('Year').alias('number_of_male_athletes'))\\\n",
    "            .sort('Year', ascending=True)\n",
    "\n",
    "## SQL Solution\n",
    "sql_res = spark.sql('''\n",
    "  SELECT year, count(*) as number_of_male_athletes\n",
    "  FROM sql_events\n",
    "  WHERE sex='M'\n",
    "  AND year >= 2000\n",
    "  GROUP BY year\n",
    "  ORDER BY year ASC\n",
    "''')\n",
    "\n",
    "# df_res_count = df_res.count()\n",
    "# print(df_res_count)\n",
    "# df_res.show(sql_res.collect())\n",
    "# sql_res_count = sql_res.count()\n",
    "# print(sql_res_count)\n",
    "print(sql_res.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-3\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">3. Lab Task: </strong> Get total number of athletes per Olympic event (summer/winter) in the 1990s decade for Australia and New Zealand. <strong>Sample Output:</strong>\n",
    "<pre>\n",
    "+-----------+------+------------------+\n",
    "|    country|season|number_of_athletes|\n",
    "+-----------+------+------------------+\n",
    "|  Australia|Summer|               XXX|\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "+-----------+------+------------------+\n",
      "|    country|season|number_of_athletes|\n",
      "+-----------+------+------------------+\n",
      "|  Australia|Summer|               940|\n",
      "|  Australia|Winter|               122|\n",
      "|New Zealand|Summer|               304|\n",
      "|New Zealand|Winter|                35|\n",
      "+-----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Dataframe Solution\n",
    "df_res = df_events.join(df_regions,df_events.NOC==df_regions.NOC,how='inner')\\\n",
    "            .filter(F.col(\"country\").isin([\"Australia\", \"New Zealand\"]))\\\n",
    "            .filter(F.col('year').between(1990,1999))\\\n",
    "            .groupBy('country','season')\\\n",
    "            .agg(F.count('country').alias('number_of_athletes'))\\\n",
    "            .sort(['country','season'], ascending=True)\n",
    "\n",
    "df_res_count = df_res.count()\n",
    "print(df_res_count)\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "+-----------+------+------------------+\n",
      "|    country|season|number_of_athletes|\n",
      "+-----------+------+------------------+\n",
      "|  Australia|Summer|               940|\n",
      "|  Australia|Winter|               122|\n",
      "|New Zealand|Summer|               304|\n",
      "|New Zealand|Winter|                35|\n",
      "+-----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Dataframe Solution\n",
    "df_res = df_events.join(df_regions,df_events.NOC==df_regions.NOC,how='inner')\\\n",
    "            .filter(F.col('year').between(1990,1999))\\\n",
    "            .filter(F.col(\"country\").isin([\"Australia\", \"New Zealand\"]))\\\n",
    "            .groupBy('country','season')\\\n",
    "            .agg(F.count('country').alias('number_of_athletes'))\\\n",
    "            .sort(['country','season'], ascending=True)\n",
    "\n",
    "df_res_count = df_res.count()\n",
    "print(df_res_count)\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------------------+\n",
      "|    country|season|number_of_athletes|\n",
      "+-----------+------+------------------+\n",
      "|  Australia|Summer|               940|\n",
      "|  Australia|Winter|               122|\n",
      "|New Zealand|Summer|               304|\n",
      "|New Zealand|Winter|                35|\n",
      "+-----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SOLUTION\n",
    "sql_res = spark.sql('''\n",
    "  SELECT country,season,count(*) as number_of_athletes\n",
    "  FROM sql_regions JOIN sql_events\n",
    "  USING (NOC)\n",
    "  WHERE (country='Australia' OR country='New Zealand')\n",
    "  AND year between 1990 and 1999\n",
    "  GROUP BY country,season\n",
    "  ORDER BY country,season\n",
    "''')\n",
    "sql_res.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-4\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>Obtain the minimum, average and maximum height of each country for the Winter Olympics and order by the average value in descending order. <strong>Output should be in the following format:</strong>\n",
    "<pre>\n",
    "+--------------------+----------+------------------+----------+\n",
    "|             country|min_height|        avg_height|max_height|\n",
    "+--------------------+----------+------------------+----------+\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "+--------------------+----------+------------------+----------+\n",
      "|             country|min_height|        avg_height|max_height|\n",
      "+--------------------+----------+------------------+----------+\n",
      "|            Cameroon|       198|             198.0|       198|\n",
      "|             Senegal|       192|             192.0|       192|\n",
      "|             Uruguay|       188|             188.0|       188|\n",
      "|        Puerto Rico*|       178|186.76923076923077|       196|\n",
      "|British Virgin Is...|       186|             186.0|       186|\n",
      "|            Bermuda*|       172|184.85714285714286|       196|\n",
      "|               Tonga|       184|             184.0|       184|\n",
      "|            Dominica|       183|             183.0|       183|\n",
      "|          San Marino|       166|182.19230769230768|       192|\n",
      "|            Zimbabwe|       182|             182.0|       182|\n",
      "| Trinidad and Tobago|       175|181.57142857142858|       193|\n",
      "|              Serbia|       167|          181.5625|       205|\n",
      "|             Jamaica|       168|181.06451612903226|       189|\n",
      "|East Timor (Timor...|       181|             181.0|       181|\n",
      "|             Ireland|       156| 180.8048780487805|       194|\n",
      "|Netherlands Antil...|       180|             180.0|       180|\n",
      "|               Ghana|       180|             180.0|       180|\n",
      "|     Cayman Islands*|       180|             180.0|       180|\n",
      "|               Kenya|       180|             180.0|       180|\n",
      "|              Latvia|       154|179.78251121076232|       197|\n",
      "+--------------------+----------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Dataframe Solution\n",
    "df_res = df_events.join(df_regions,df_events.NOC==df_regions.NOC,how='inner')\\\n",
    "            .filter( (F.col(\"season\")=='Winter') & (F.col(\"height\")!='NA') )\\\n",
    "            .groupBy('country')\\\n",
    "            .agg(F.min('height').alias('min_height'),\n",
    "                F.avg('height').alias('avg_height'),\n",
    "                F.max('height').alias('max_height'))\\\n",
    "            .sort('avg_height', ascending=False)\n",
    "\n",
    "df_res_count = df_res.count()\n",
    "print(df_res_count)\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+----------+\n",
      "|             country|min_height|        avg_height|max_height|\n",
      "+--------------------+----------+------------------+----------+\n",
      "|            Cameroon|       198|             198.0|       198|\n",
      "|             Senegal|       192|             192.0|       192|\n",
      "|             Uruguay|       188|             188.0|       188|\n",
      "|        Puerto Rico*|       178|186.76923076923077|       196|\n",
      "|British Virgin Is...|       186|             186.0|       186|\n",
      "|            Bermuda*|       172|184.85714285714286|       196|\n",
      "|               Tonga|       184|             184.0|       184|\n",
      "|            Dominica|       183|             183.0|       183|\n",
      "|          San Marino|       166|182.19230769230768|       192|\n",
      "|            Zimbabwe|       182|             182.0|       182|\n",
      "| Trinidad and Tobago|       175|181.57142857142858|       193|\n",
      "|              Serbia|       167|          181.5625|       205|\n",
      "|             Jamaica|       168|181.06451612903226|       189|\n",
      "|East Timor (Timor...|       181|             181.0|       181|\n",
      "|             Ireland|       156| 180.8048780487805|       194|\n",
      "|Netherlands Antil...|       180|             180.0|       180|\n",
      "|               Ghana|       180|             180.0|       180|\n",
      "|     Cayman Islands*|       180|             180.0|       180|\n",
      "|               Kenya|       180|             180.0|       180|\n",
      "|              Latvia|       154|179.78251121076232|       197|\n",
      "+--------------------+----------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SOLUTION\n",
    "sql_res = spark.sql('''\n",
    "  SELECT country,min(height) as min_height,\n",
    "  avg(height) as avg_height, max(height) as max_height\n",
    "  FROM sql_regions JOIN sql_events\n",
    "  USING (NOC)\n",
    "  WHERE season = 'Winter'\n",
    "  AND height is not null AND height != 'NA'\n",
    "  GROUP BY country\n",
    "  ORDER BY avg_height DESC\n",
    "''')\n",
    "sql_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-5\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong> Get the Olympics teams that don't have information of their countries in noc_regions (e.g. Soviet Union since it doesn't exist anymore). <strong>Output should be in the following format:</strong>\n",
    "<pre>\n",
    "+--------------------+---+\n",
    "|                team|noc|\n",
    "+--------------------+---+\n",
    "|               Almaz|URS|\n",
    "|         Australasia|ANZ|\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_regions.withColumnRenamed('NOC', 'noc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "+--------------------+---+\n",
      "|                team|noc|\n",
      "+--------------------+---+\n",
      "|               Almaz|URS|\n",
      "|         Australasia|ANZ|\n",
      "|             Bohemia|BOH|\n",
      "|           Bohemia-1|BOH|\n",
      "|           Bohemia-2|BOH|\n",
      "|           Bohemia-3|BOH|\n",
      "|Bohemia/Great Bri...|BOH|\n",
      "|         Burevestnik|URS|\n",
      "|         Cha-Cha III|YUG|\n",
      "|              Circus|WIF|\n",
      "|               Crete|CRT|\n",
      "|      Czechoslovakia|TCH|\n",
      "|    Czechoslovakia-1|TCH|\n",
      "|    Czechoslovakia-2|TCH|\n",
      "|    Czechoslovakia-3|TCH|\n",
      "|             Druzhba|URS|\n",
      "|        East Germany|GDR|\n",
      "|      East Germany-1|GDR|\n",
      "|      East Germany-2|GDR|\n",
      "|      East Germany-3|GDR|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Dataframe Solution\n",
    "df_res = df_events.join(df2,df_events.NOC==df2.noc2,how='left')\\\n",
    "            .filter(F.col('noc2').isNull())\\\n",
    "            .select('team','noc').distinct()\\\n",
    "            .sort('team', ascending=True)\n",
    "\n",
    "df_res_count = df_res.count()\n",
    "print(df_res_count)\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                team|noc|\n",
      "+--------------------+---+\n",
      "|               Almaz|URS|\n",
      "|         Australasia|ANZ|\n",
      "|             Bohemia|BOH|\n",
      "|           Bohemia-1|BOH|\n",
      "|           Bohemia-2|BOH|\n",
      "|           Bohemia-3|BOH|\n",
      "|Bohemia/Great Bri...|BOH|\n",
      "|         Burevestnik|URS|\n",
      "|         Cha-Cha III|YUG|\n",
      "|              Circus|WIF|\n",
      "|               Crete|CRT|\n",
      "|      Czechoslovakia|TCH|\n",
      "|    Czechoslovakia-1|TCH|\n",
      "|    Czechoslovakia-2|TCH|\n",
      "|    Czechoslovakia-3|TCH|\n",
      "|             Druzhba|URS|\n",
      "|        East Germany|GDR|\n",
      "|      East Germany-1|GDR|\n",
      "|      East Germany-2|GDR|\n",
      "|      East Germany-3|GDR|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SOLUTION\n",
    "sql_res = spark.sql('''\n",
    "  SELECT distinct team,e.noc\n",
    "  FROM sql_events e LEFT JOIN sql_regions r\n",
    "  USING (NOC)\n",
    "  WHERE r.noc is null\n",
    "  ORDER BY team\n",
    "''')\n",
    "sql_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,255,0,0.2);padding:10px;border-radius:4px\">\n",
    " <h3>Assignment 1</h3>\n",
    "    Once you are done with the lab tasks, please work on your Assignment 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4Vj16RFIgk"
   },
   "source": [
    "**Congratulations on finishing this activity!**\n",
    "\n",
    "Having practiced today's activities, we're now ready to embark on a trip of the rest of exiciting FIT5202 activities! See you next week!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yVWYWwzMFIfR",
    "K2QtBnKgFIfa",
    "kL88Q46yFIfh",
    "48_7UVktFIgD",
    "dtN67ydpFIgF",
    "cSs0qd02FIgI"
   ],
   "name": "FIT5202 - Parallel Aggregation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
