{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Assignment 2A : Building Models for Building Energy Prediction\n",
    "\n",
    "## Table of Contents\n",
    "*  \n",
    "    * [Part 1 : Data Loading, Transformation and Exploration](#part-1)\n",
    "    * [Part 2 : Feature extraction and ML training](#part-2)\n",
    "    * [Part 3 : Hyperparameter Tuning and Model Optimisation](#part-3)  \n",
    "Please add code/markdown cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Loading, Transformation and Exploration <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Data Loading\n",
    "In this section, you must load the given datasets into PySpark DataFrames and use DataFrame functions to process the data. For plotting, various visualisation packages can be used, but please ensure that you have included instructions to install the additional packages and that the installation will be successful in the provided Docker container (in case your marker needs to clear the notebook and rerun it).\n",
    "\n",
    "### 1.1.1 Data Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.1.1 Write the code to create a SparkSession. For creating the SparkSession, you need to use a SparkConf object to configure the Spark app with a proper application name, to ensure the maximum partition size does not exceed 32MB, and to run locally with all CPU cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FIT5202 Assignment 2A\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", 32 * 1024 * 1024) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Write code to define the schemas for the datasets, following the data types suggested in the metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, CharType\n",
    "\n",
    "meter_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"meter_type\", StringType(), True), # Reading as String is safer than CharType\n",
    "    StructField(\"ts\", TimestampType(), True),\n",
    "    StructField(\"value\", DoubleType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "building_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), True),\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"primary_use\", StringType(), True),\n",
    "    StructField(\"square_feet\", IntegerType(), True),\n",
    "    StructField(\"floor_count\", IntegerType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), True),\n",
    "    StructField(\"year_built\", IntegerType(), True),\n",
    "    StructField(\"latent_y\", DoubleType(), True),\n",
    "    StructField(\"latent_s\", DoubleType(), True),\n",
    "    StructField(\"latent_r\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema for weather.csv\n",
    "# We match the exact order from the header, noting precip_depth_1_hr is missing.\n",
    "weather_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"air_temperature\", DoubleType(), True),\n",
    "    StructField(\"cloud_coverage\", DoubleType(), True),\n",
    "    StructField(\"dew_temperature\", DoubleType(), True),\n",
    "    StructField(\"sea_level_pressure\", DoubleType(), True),\n",
    "    StructField(\"wind_direction\", DoubleType(), True),\n",
    "    StructField(\"wind_speed\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 Using your schemas, load the CSV files into separate data frames. Print the schemas of all data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correctly Loaded building_df ---\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- primary_use: string (nullable = true)\n",
      " |-- square_feet: integer (nullable = true)\n",
      " |-- floor_count: integer (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      " |-- latent_y: double (nullable = true)\n",
      " |-- latent_s: double (nullable = true)\n",
      " |-- latent_r: double (nullable = true)\n",
      "\n",
      "+-------+-----------+-------------+-----------+-----------+------+----------+--------+---------+--------+\n",
      "|site_id|building_id|  primary_use|square_feet|floor_count|row_id|year_built|latent_y| latent_s|latent_r|\n",
      "+-------+-----------+-------------+-----------+-----------+------+----------+--------+---------+--------+\n",
      "|      2|        165|    Warehouse|       3877|          1|   166|      1982|    18.0|3.5884957|     3.0|\n",
      "|      2|        229|    Education|     140092|          1|   230|      1999|     1.0|5.1464133|     3.0|\n",
      "|      1|        121|    Education|     150318|          9|   122|      1913|    87.0|4.2227683|     2.0|\n",
      "|      8|        871|Entertainment|       1536|          1|   872|      1974|    26.0| 3.186391|     4.0|\n",
      "|      9|        939|  Residential|       8051|          1|   940|      1980|    20.0|  3.90585|     4.0|\n",
      "+-------+-----------+-------------+-----------+-----------+------+----------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--- Correctly Loaded weather_df ---\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- air_temperature: double (nullable = true)\n",
      " |-- cloud_coverage: double (nullable = true)\n",
      " |-- dew_temperature: double (nullable = true)\n",
      " |-- sea_level_pressure: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      "\n",
      "+-------+-------------------+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "|site_id|          timestamp|air_temperature|cloud_coverage|dew_temperature|sea_level_pressure|wind_direction|wind_speed|\n",
      "+-------+-------------------+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "|      0|2022-01-01 22:00:00|           26.7|          NULL|           18.3|            1016.9|         230.0|       3.1|\n",
      "|      0|2022-01-01 23:00:00|           25.6|          NULL|           18.3|            1017.5|         230.0|       3.1|\n",
      "|      0|2022-01-02 00:00:00|           24.4|           6.0|           18.9|            1018.1|         270.0|       2.6|\n",
      "|      0|2022-01-02 01:00:00|           23.9|           4.0|           18.3|            1018.5|         300.0|       2.1|\n",
      "|      0|2022-01-02 02:00:00|           22.2|          NULL|           19.4|              NULL|         360.0|       5.7|\n",
      "+-------+-------------------+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--- Correctly Loaded meter_df ---\n",
      "root\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- meter_type: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      "\n",
      "+-----------+----------+-------------------+-------+------+\n",
      "|building_id|meter_type|                 ts|  value|row_id|\n",
      "+-----------+----------+-------------------+-------+------+\n",
      "|        161|         c|2022-01-01 00:00:00|    0.0|     1|\n",
      "|        162|         c|2022-01-01 00:00:00|    0.0|     2|\n",
      "|        166|         c|2022-01-01 00:00:00|209.886|     4|\n",
      "|        167|         c|2022-01-01 00:00:00|    0.0|     5|\n",
      "|        168|         c|2022-01-01 00:00:00| 51.557|     6|\n",
      "+-----------+----------+-------------------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "building_df = spark.read.csv(\"dataset/building_information.csv\", header=True, schema=building_schema)\n",
    "weather_df = spark.read.csv(\"dataset/weather.csv\", header=True, schema=weather_schema)\n",
    "meter_df = spark.read.csv(\"dataset/meters.csv\", header=True, schema=meter_schema)\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"--- Correctly Loaded building_df ---\")\n",
    "building_df.printSchema()\n",
    "building_df.show(5)\n",
    "\n",
    "print(\"\\n--- Correctly Loaded weather_df ---\")\n",
    "weather_df.printSchema()\n",
    "weather_df.show(5)\n",
    "\n",
    "print(\"\\n--- Correctly Loaded meter_df ---\")\n",
    "meter_df.printSchema()\n",
    "meter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Transformation to Create Features <a class=\"anchor\" name=\"1.2\"></a>\n",
    "In this section, we primarily have three tasks:  \n",
    "1.2.1 The dataset includes sensors with hourly energy measurements. However, as a grid operator, we don’t need this level of granularity and lowering it can reduce the amount of data we need to process. For each building, we will aggregate the metered energy consumption in 6-hour intervals (0:00-5:59, 6:00-11:59, 12:00-17:59, 18:00-23:59). This will be our target (label) column for this prediction. Perform the aggregation for each building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Aggregated Meter Readings (6-hour intervals) ---\n",
      "+-----------+--------------------+------------------------+\n",
      "|building_id|              window|total_energy_consumption|\n",
      "+-----------+--------------------+------------------------+\n",
      "|        258|{2022-01-01 12:00...|      242.17629999999997|\n",
      "|        928|{2022-01-01 12:00...|                4054.541|\n",
      "|        996|{2022-01-01 12:00...|               2030.5453|\n",
      "|       1168|{2022-01-02 12:00...|             308836.8331|\n",
      "|        920|{2022-01-02 18:00...|       576.0677000000001|\n",
      "|       1342|{2022-01-02 18:00...|                903.0262|\n",
      "|       1388|{2022-01-03 06:00...|                478.5848|\n",
      "|       1211|{2022-01-03 18:00...|               12046.066|\n",
      "|        801|{2022-01-04 00:00...|       48967.21000000001|\n",
      "|       1312|{2022-01-04 00:00...|               2476.4169|\n",
      "|       1088|{2022-01-04 06:00...|             153586.5986|\n",
      "|       1169|{2022-01-04 12:00...|               31927.523|\n",
      "|       1355|{2022-01-04 12:00...|       6878.992300000001|\n",
      "|       1358|{2022-01-04 12:00...|               7549.1102|\n",
      "|       1388|{2022-01-04 12:00...|      345.62649999999996|\n",
      "|        968|{2022-01-04 18:00...|               2931.7625|\n",
      "|        219|{2022-01-05 00:00...|      2612.4966999999997|\n",
      "|        885|{2022-01-05 00:00...|                608.6584|\n",
      "|        975|{2022-01-05 00:00...|               2551.3188|\n",
      "|       1167|{2022-01-05 00:00...|                1990.142|\n",
      "+-----------+--------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, sum as _sum\n",
    "\n",
    "# --- 1.2.1 Aggregate Meter Readings ---\n",
    "\n",
    "# We will group by the building_id and a 6-hour window on the 'ts' (timestamp) column.\n",
    "# Then, we will sum the 'value' column to get the total energy consumption for that period.\n",
    "# Note: We alias the sum function to avoid conflict with the Python built-in sum.\n",
    "meter_df_agg = meter_df.groupBy(\n",
    "    \"building_id\",\n",
    "    window(\"ts\", \"6 hours\")\n",
    ").agg(\n",
    "    _sum(\"value\").alias(\"total_energy_consumption\")\n",
    ")\n",
    "\n",
    "# Show the results. This is our target (label) column for the ML model.\n",
    "print(\"--- Aggregated Meter Readings (6-hour intervals) ---\")\n",
    "meter_df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the weather dataset, there are some missing values (null or empty strings). It may lower the quality of our model. Imputation is a way to deal with those missing values. Imputation is the process of replacing missing values in a dataset with substituted, or \"imputed,\" values. It's a way to handle gaps in your data so that you can still analyse it effectively without having to delete incomplete records.  \n",
    "1.2.2 Refer to the Spark MLLib imputation API and fill in the missing values in the weather dataset. You can use mean values as the strategy.  https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.ml.feature.Imputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Null counts in weather_df after imputation ---\n",
      "+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "|air_temperature|cloud_coverage|dew_temperature|sea_level_pressure|wind_direction|wind_speed|\n",
      "+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "|              0|             0|              0|                 0|             0|         0|\n",
      "+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "\n",
      "\n",
      "--- Cleaned Weather Data ---\n",
      "+-------+-------------------+---------------+------------------+---------------+------------------+--------------+----------+\n",
      "|site_id|          timestamp|air_temperature|    cloud_coverage|dew_temperature|sea_level_pressure|wind_direction|wind_speed|\n",
      "+-------+-------------------+---------------+------------------+---------------+------------------+--------------+----------+\n",
      "|      0|2022-01-01 22:00:00|           26.7|2.1493059490084985|           18.3|            1016.9|         230.0|       3.1|\n",
      "|      0|2022-01-01 23:00:00|           25.6|2.1493059490084985|           18.3|            1017.5|         230.0|       3.1|\n",
      "|      0|2022-01-02 00:00:00|           24.4|               6.0|           18.9|            1018.1|         270.0|       2.6|\n",
      "|      0|2022-01-02 01:00:00|           23.9|               4.0|           18.3|            1018.5|         300.0|       2.1|\n",
      "|      0|2022-01-02 02:00:00|           22.2|2.1493059490084985|           19.4|1016.1580380163365|         360.0|       5.7|\n",
      "|      0|2022-01-02 03:00:00|           21.1|2.1493059490084985|           18.9|            1019.5|          20.0|       5.1|\n",
      "|      0|2022-01-02 04:00:00|           20.6|2.1493059490084985|           17.8|            1019.4|          30.0|       4.6|\n",
      "|      0|2022-01-02 05:00:00|           19.4|               4.0|           17.2|            1019.3|          20.0|       2.6|\n",
      "|      0|2022-01-02 06:00:00|           18.9|               6.0|           17.2|            1019.0|          10.0|       2.1|\n",
      "|      0|2022-01-02 07:00:00|           18.9|2.1493059490084985|           17.2|            1018.4|          10.0|       2.6|\n",
      "|      0|2022-01-02 08:00:00|           18.9|2.1493059490084985|           16.7|            1018.5|         360.0|       3.6|\n",
      "|      0|2022-01-02 09:00:00|           18.3|2.1493059490084985|           16.7|            1018.1|          10.0|       3.1|\n",
      "|      0|2022-01-02 10:00:00|           18.9|2.1493059490084985|           16.7|            1018.1|          10.0|       5.1|\n",
      "|      0|2022-01-02 11:00:00|           16.7|2.1493059490084985|           13.9|            1018.7|          20.0|       6.2|\n",
      "|      0|2022-01-02 12:00:00|           15.6|               8.0|           13.3|            1019.5|         350.0|       5.7|\n",
      "|      0|2022-01-02 13:00:00|           15.0|2.1493059490084985|           13.9|            1020.0|         360.0|       4.1|\n",
      "|      0|2022-01-02 14:00:00|           16.1|2.1493059490084985|           13.3|            1020.9|         360.0|       5.1|\n",
      "|      0|2022-01-02 15:00:00|           17.2|2.1493059490084985|           13.3|            1021.5|         360.0|       3.6|\n",
      "|      0|2022-01-02 16:00:00|           17.8|2.1493059490084985|           13.3|            1021.5|         350.0|       3.6|\n",
      "|      0|2022-01-02 17:00:00|           19.4|2.1493059490084985|           11.7|            1020.4|         360.0|       3.6|\n",
      "+-------+-------------------+---------------+------------------+---------------+------------------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# --- 1.2.2 Impute Missing Values in Weather Data ---\n",
    "\n",
    "# Columns that need imputation.\n",
    "impute_cols = [\n",
    "    \"air_temperature\",\n",
    "    \"cloud_coverage\",\n",
    "    \"dew_temperature\",\n",
    "    \"sea_level_pressure\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\"\n",
    "]\n",
    "\n",
    "# Create the Imputer with the 'mean' strategy.\n",
    "imputer = Imputer(\n",
    "    inputCols=impute_cols,\n",
    "    outputCols=[f\"{c}_imputed\" for c in impute_cols]\n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "# Fit the imputer to the data to learn the means.\n",
    "imputer_model = imputer.fit(weather_df)\n",
    "\n",
    "# Transform the data to fill the NULLs.\n",
    "weather_df_imputed = imputer_model.transform(weather_df)\n",
    "\n",
    "# Clean up the dataframe by dropping the original columns and renaming the imputed ones.\n",
    "weather_df_cleaned = weather_df_imputed\n",
    "for c in impute_cols:\n",
    "    weather_df_cleaned = weather_df_cleaned.drop(c).withColumnRenamed(f\"{c}_imputed\", c)\n",
    "\n",
    "# Verify that our imputation worked. The null counts should now be zero.\n",
    "print(\"--- Null counts in weather_df after imputation ---\")\n",
    "weather_df_cleaned.select([count(when(col(c).isNull(), c)).alias(c) for c in impute_cols]).show()\n",
    "\n",
    "print(\"\\n--- Cleaned Weather Data ---\")\n",
    "weather_df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  m|count|\n",
      "+---+-----+\n",
      "|  1|  744|\n",
      "|  2|  696|\n",
      "|  3|  744|\n",
      "|  4|  720|\n",
      "|  5|  744|\n",
      "|  6|  720|\n",
      "|  7|  744|\n",
      "|  8|  744|\n",
      "|  9|  720|\n",
      "| 10|  744|\n",
      "| 11|  720|\n",
      "| 12|  744|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, col\n",
    "\n",
    "# 1) Do we have all 12 months for site 0 in the base (cleaned/imputed) weather DF?\n",
    "weather_df_cleaned.filter(col(\"site_id\")==0) \\\n",
    "    .select(month(\"timestamp\").alias(\"m\")) \\\n",
    "    .groupBy(\"m\").count().orderBy(\"m\").show(100)\n",
    "\n",
    "# Expect months 1..12 present with non-zero counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that different seasons may affect energy consumption—for instance, a heater in winter and a cooler in summer. Extracting peak seasons (summer and winter) or off-peak seasons (Spring and Autumn) might be more useful than directly using the month as numerical values.   \n",
    "1.2.3 The dataset has 16 sites in total, whose locations may span across different countries. Add a column (peak/off-peak) to the weather data frame based on the average air temperature. The top 3 hottest months and the 3 coldest months are considered “peak”, and the rest of the year is considered “off-peak”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- Base alias (must be the full cleaned weather DF; do NOT pre-filter it) ---\n",
    "w = weather_df_cleaned.alias(\"w\")\n",
    "\n",
    "# 1) Monthly avg temperature per site\n",
    "monthly_avg = (\n",
    "    w.withColumn(\"month\", F.month(\"w.timestamp\"))\n",
    "     .groupBy(\"w.site_id\", \"month\")\n",
    "     .agg(F.avg(\"w.air_temperature\").alias(\"avg_temp\"))\n",
    "     .alias(\"ma\")\n",
    ")\n",
    "\n",
    "# 2) Tie-safe top-3 hottest and top-3 coldest months per site using row_number\n",
    "win_cold = Window.partitionBy(\"ma.site_id\").orderBy(F.col(\"avg_temp\").asc())\n",
    "win_hot  = Window.partitionBy(\"ma.site_id\").orderBy(F.col(\"avg_temp\").desc())\n",
    "\n",
    "cold3 = (\n",
    "    monthly_avg\n",
    "    .withColumn(\"cold_rank\", F.row_number().over(win_cold))\n",
    "    .filter(F.col(\"cold_rank\") <= 3)\n",
    "    .select(F.col(\"ma.site_id\").alias(\"site_id\"), \"month\")\n",
    ")\n",
    "\n",
    "hot3 = (\n",
    "    monthly_avg\n",
    "    .withColumn(\"hot_rank\", F.row_number().over(win_hot))\n",
    "    .filter(F.col(\"hot_rank\") <= 3)\n",
    "    .select(F.col(\"ma.site_id\").alias(\"site_id\"), \"month\")\n",
    ")\n",
    "\n",
    "peak_months = (\n",
    "    cold3.unionByName(hot3)\n",
    "         .dropDuplicates([\"site_id\", \"month\"])\n",
    "         .alias(\"pm\")\n",
    ")\n",
    "\n",
    "# 3) LEFT join back to full weather to label season_type\n",
    "weather_with_season = (\n",
    "    w.join(\n",
    "        peak_months,\n",
    "        on=(\n",
    "            (F.col(\"w.site_id\") == F.col(\"pm.site_id\")) &\n",
    "            (F.month(F.col(\"w.timestamp\")) == F.col(\"pm.month\"))\n",
    "        ),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"season_type\",\n",
    "        F.when(F.col(\"pm.month\").isNotNull(), F.lit(\"peak\")).otherwise(F.lit(\"off-peak\"))\n",
    "    )\n",
    "    .drop(\"pm.site_id\", \"pm.month\")\n",
    ")\n",
    "\n",
    "# (Optional) keep a clean projection (nice for downstream joins)\n",
    "weather_with_season = weather_with_season.select(\n",
    "    \"w.timestamp\", \"w.site_id\",\n",
    "    \"w.air_temperature\", \"w.cloud_coverage\", \"w.dew_temperature\",\n",
    "    \"w.sea_level_pressure\", \"w.wind_direction\", \"w.wind_speed\",\n",
    "    \"season_type\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation 0: Average Monthly Temperature for Site 0 ---\n",
      "+-------+-----+------------------+\n",
      "|site_id|month|          avg_temp|\n",
      "+-------+-----+------------------+\n",
      "|      0|    1|14.713110644374733|\n",
      "|      0|    2| 16.13965517241379|\n",
      "|      0|    3|  21.2662634408602|\n",
      "|      0|    4| 22.43125000000002|\n",
      "|      0|    5|  24.7342741935484|\n",
      "|      0|    6|27.366388888888856|\n",
      "|      0|    7| 28.55282258064521|\n",
      "|      0|    8|27.613575268817204|\n",
      "|      0|    9|26.871944444444473|\n",
      "|      0|   10| 24.03817204301075|\n",
      "|      0|   11|20.055416666666662|\n",
      "|      0|   12|19.956989247311842|\n",
      "+-------+-----+------------------+\n",
      "\n",
      "\n",
      "--- Validation 1: Peak/Off-Peak Row Count for Site 0 ---\n",
      "+-----------+-----+\n",
      "|season_type|count|\n",
      "+-----------+-----+\n",
      "|   off-peak| 4392|\n",
      "|       peak| 4392|\n",
      "+-----------+-----+\n",
      "\n",
      "\n",
      "--- Validation 2: Count of Peak/Off-Peak Months Per Site ---\n",
      "+-------+-----------+-----+\n",
      "|site_id|season_type|count|\n",
      "+-------+-----------+-----+\n",
      "|      0|   off-peak|    6|\n",
      "|      0|       peak|    6|\n",
      "|      1|   off-peak|    6|\n",
      "|      1|       peak|    6|\n",
      "|      2|   off-peak|    6|\n",
      "|      2|       peak|    6|\n",
      "|      3|   off-peak|    6|\n",
      "|      3|       peak|    6|\n",
      "|      4|   off-peak|    6|\n",
      "|      4|       peak|    6|\n",
      "|      5|   off-peak|    6|\n",
      "|      5|       peak|    6|\n",
      "|      6|   off-peak|    6|\n",
      "|      6|       peak|    6|\n",
      "|      7|   off-peak|    6|\n",
      "|      7|       peak|    6|\n",
      "|      8|   off-peak|    6|\n",
      "|      8|       peak|    6|\n",
      "|      9|   off-peak|    6|\n",
      "|      9|       peak|    6|\n",
      "|     10|   off-peak|    6|\n",
      "|     10|       peak|    6|\n",
      "|     11|   off-peak|    6|\n",
      "|     11|       peak|    6|\n",
      "|     12|   off-peak|    6|\n",
      "|     12|       peak|    6|\n",
      "|     13|   off-peak|    6|\n",
      "|     13|       peak|    6|\n",
      "|     14|   off-peak|    6|\n",
      "|     14|       peak|    6|\n",
      "|     15|   off-peak|    6|\n",
      "|     15|       peak|    6|\n",
      "+-------+-----------+-----+\n",
      "\n",
      "\n",
      "--- Validation 3: Total Distinct Months Per Site ---\n",
      "+-------+-----+\n",
      "|site_id|count|\n",
      "+-------+-----+\n",
      "|      0|   12|\n",
      "|      1|   12|\n",
      "|      2|   12|\n",
      "|      3|   12|\n",
      "|      4|   12|\n",
      "|      5|   12|\n",
      "|      6|   12|\n",
      "|      7|   12|\n",
      "|      8|   12|\n",
      "|      9|   12|\n",
      "|     10|   12|\n",
      "|     11|   12|\n",
      "|     12|   12|\n",
      "|     13|   12|\n",
      "|     14|   12|\n",
      "|     15|   12|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- VALIDATION SUITE ---\n",
    "\n",
    "# 0) VALIDATE THE MONTHLY AVERAGE TEMPERATURES (NEW)\n",
    "# This is the foundational data that the ranking is based on.\n",
    "print(\"--- Validation 0: Average Monthly Temperature for Site 0 ---\")\n",
    "monthly_avg.filter(F.col(\"site_id\") == 0).orderBy(\"month\").show()\n",
    "\n",
    "\n",
    "# 1) Site 0 must have BOTH labels now\n",
    "print(\"\\n--- Validation 1: Peak/Off-Peak Row Count for Site 0 ---\")\n",
    "weather_with_season.filter(F.col(\"site_id\")==0) \\\n",
    "    .groupBy(\"season_type\").count().show()\n",
    "\n",
    "# 2) Exactly 6 peak months per site (months, not rows)\n",
    "print(\"\\n--- Validation 2: Count of Peak/Off-Peak Months Per Site ---\")\n",
    "months_by_label = (weather_with_season\n",
    "    .select(\"site_id\", F.month(\"timestamp\").alias(\"m\"), \"season_type\")\n",
    "    .dropDuplicates([\"site_id\",\"m\",\"season_type\"])\n",
    "    .groupBy(\"site_id\",\"season_type\").count()\n",
    ")\n",
    "months_by_label.orderBy(\"site_id\",\"season_type\").show(50)\n",
    "\n",
    "# 3) Spot-check distinct months per site are 12\n",
    "print(\"\\n--- Validation 3: Total Distinct Months Per Site ---\")\n",
    "distinct_months = (weather_with_season\n",
    "    .select(\"site_id\", F.month(\"timestamp\").alias(\"m\"))\n",
    "    .dropDuplicates()\n",
    "    .groupBy(\"site_id\").count()\n",
    ")\n",
    "distinct_months.orderBy(\"site_id\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with all relevant columns at this stage, we refer to this data frame as feature_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring the Data <a class=\"anchor\" name=\"1.3\"></a>\n",
    "You can use either the CDA or the EDA method mentioned in Lab 5.  \n",
    "Some ideas for CDA:  \n",
    "a)\tOlder building may not be as efficient as new ones, therefore need more energy for cooling/heating. It’s not necessarily true though, if the buildings are built with higher standard or renovated later.  \n",
    "b)\tA multifloored or larger building obviously consumes more energy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWith the feature_df, write code to show the basic statistics:  \n",
    "a) For each numeric column, show count, mean, stddev, min, max, 25 percentile, 50 percentile, 75 percentile;  \n",
    "b) For each non-numeric column, display the top-5 values and the corresponding counts;  \n",
    "c) For each boolean column, display the value and count. (note: pandas describe is allowed for this task.) (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tExplore the dataframe and write code to present two plots of multivariate analysis, describe your plots and discuss the findings from the plots. (5% each).  \n",
    "○\t150 words max for each plot’s description and discussion.  \n",
    "○\tNote: In the building metadata table, there are some latent columns (data that may or may not be helpful, their meanings is unknown due to privacy and data security concerns).  \n",
    "○\tFeel free to use any plotting libraries: matplotlib, seabon, plotly, etc. You can refer to https://samplecode.link  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Feature extraction and ML training <a class=\"anchor\" name=\"part-2\"></a>\n",
    "In this section, you must use PySpark DataFrame functions and ML packages for data preparation, model building, and evaluation. Other ML packages, such as scikit-learn, should not be used to process the data; however, it’s fine to use them to display the result or evaluate your model.  \n",
    "### 2.1 Discuss the feature selection and prepare the feature columns\n",
    "\n",
    "2.1.1 Based on the data exploration from 1.2 and considering the use case, discuss the importance of those features (For example, which features may be useless and should be removed, which feature has a significant impact on the label column, which should be transformed), which features you are planning to use? Discuss the reasons for selecting them and how you plan to create/transform them.  \n",
    "○\t300 words max for the discussion  \n",
    "○\tPlease only use the provided data for model building  \n",
    "○\tYou can create/add additional features based on the dataset  \n",
    "○\tHint - Use the insights from the data exploration/domain knowledge/statistical models to consider whether to create more feature columns, whether to remove some columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Write code to create/transform the columns based on your discussion above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing Spark ML Transformers/Estimators for features, labels, and models  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "\n",
    "**2.2.1 Write code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1 and create ML model Estimators for Random Forest (RF) and Gradient-boosted tree (GBT) model.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2. Write code to include the above Transformers/Estimators into two pipelines.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the training data and testing data  \n",
    "Write code to split the data for training and testing, using 2025 as the random seed. You can decide the train/test split ratio based on the resources available on your laptop.  \n",
    "Note: Due to the large dataset size, you can use random sampling (say 20% of the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and evaluating models  \n",
    "2.4.1 Write code to use the corresponding ML Pipelines to train the models on the training data from 2.3. And then use the trained models to predict the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 For both models (RF and GBT): with the test data, decide on which metrics to use for model evaluation and discuss which one is the better model (no word limit; please keep it concise). You may also use a plot for visualisation (not mandatory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 3.\tSave the better model (you’ll need it for A2B).\n",
    "(Note: You may need to go through a few training loops or use more data to create a better-performing model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Hyperparameter Tuning and Model Optimisation <a class=\"anchor\" name=\"part-3\"></a>  \n",
    "Apply the techniques you have learnt from the labs, for example, CrossValidator, TrainValidationSplit, ParamGridBuilder, etc., to perform further hyperparameter tuning and model optimisation.  \n",
    "The assessment is based on the quality of your work/process, not the quality of your model. Please include your thoughts/ideas/discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "Please add your references below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
