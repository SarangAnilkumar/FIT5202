{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Data processing for big data\n",
    "\n",
    "## Activity: Machine Learning with Spark (Clustering using K-Means)\n",
    "\n",
    "This week we are going to look into clustering using K-Means alogrithm. We will look into the case study where <strong>we use machine learning to identify the involvement of the attackers</strong>.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [K-Means Clustering](#one)\n",
    "* [K-Means Clustering-DEMO](#demo)\n",
    "* [Use Case : Problem Statement](#problem-statement)\n",
    "    * [Data Loading](#loading)\n",
    "    * [Data Preparation](#preparation)\n",
    "    * [Feature Engineering](#fe)\n",
    "    * [Clustering](#clustering)\n",
    "    * [Silhouette Score ](#ss)    \n",
    "* [Optimal Number of Clusters](#optimal)\n",
    "* [Lab Tasks](#lab-task-1)\n",
    "    * [Lab Task 1](#lab-task-1)\n",
    "    * [Lab Task 2](#lab-task-2)\n",
    "    * [Lab Task 3](#lab-task-3)\n",
    "    * [Lab Task 4](#lab-task-4)\n",
    "\n",
    "\n",
    "## K-Means Clustering <a class=\"anchor\" name=\"one\"></a>\n",
    "\n",
    "Cluster analysis is an area of machine learning that focuses on finding patterns in unlabelled data. The main idea behind clustering is to group similar kinds of data together into clusters, in hopes of creating useful labels for these groups. This field is also known as `unsupervised learning`.\n",
    "\n",
    "When supervised algorithms are used more for making predictions, unsupervised algorithms would be more useful for exploring data.\n",
    "`K-means` is one of the simplest unsupervised algorithms used today, and it can help partition a set of **n** observations into **k** clusters or groups.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering DEMO <a class=\"anchor\" name=\"demo\"></a>\n",
    "The following example shows the implementation of K-Means clustering for a simple dataset with three colummns i.e. <code>email,income and gender.</code> The pipeline used here is very familier to what we have been doing since the last 2 labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-1\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">1. Lab Task: </strong> \n",
    "Try to understand the steps shown in the example below. Briefly explain the pipeline implementation to your tutor to demonstrate your understanding about:\n",
    "    <ul>\n",
    "<li>1. The use of 3 different transformers for feature engineering</li>\n",
    "<li>2. How pipeline API is is used to organize the steps.</li>\n",
    "    </ul>\n",
    "Also discuss about the <strong>Predictions</strong> and the <strong>Silhouette score</strong> that you see.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Create Spark Session\n",
    "spark = SparkSession.builder.appName('Clustering using K-Means').getOrCreate()\n",
    "\n",
    "#Step 1 : Prepare the Data\n",
    "df = spark.createDataFrame(\n",
    " [(\"a@email.com\", 12000,\"M\"),\n",
    "    (\"b@email.com\", 43000,\"M\"),\n",
    "    (\"c@email.com\", 5000,\"F\"),\n",
    "    (\"d@email.com\", 60000,\"M\"),\n",
    "    (\"e@email.com\", 55000,\"M\"),\n",
    "    (\"f@email.com\", 11000,\"F\")\n",
    " ],\n",
    " [\"email\",'income','gender'])\n",
    "\n",
    "#Step 2 : Feature Engineering \n",
    "indexer = StringIndexer(inputCols=['email','gender'],outputCols=['email_index','output_index'])\n",
    "encoder = OneHotEncoder(inputCols=['email_index','output_index'],outputCols=['email_vec','output_vec'])\n",
    "assembler = VectorAssembler(inputCols=['email_vec','output_vec','income'],outputCol='features')\n",
    "#Create a KMeans Model Estimator initialized with 2 clusters\n",
    "k_means = KMeans(featuresCol='features', k=2)\n",
    "\n",
    "#Step 3 : Pipeline API and ML Model\n",
    "pipeline = Pipeline(stages = [indexer,encoder,assembler,k_means])\n",
    "pipelineModel = pipeline.fit(df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipelineModel.transform(df)\n",
    "predictions.show()\n",
    "# # Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = pipelineModel.stages[-1].clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem Statement <a class=\"anchor\" name=\"problem-statement\"></a>\n",
    "\n",
    "RhinoTech is a large technology firm that has been recently hacked. Luckily, the forensic engineers at the company have been able to grab metadata about each session used by the hackers to connect to RhinoTech servers. This data includes information such as session time, locations, words-per-minute typing speed, etc.\n",
    "You have been informed that there are <strong style=\"color:red\">three potential hackers that perpetrated the attack.     The RhinoTech forensic team are certain that the first two hackers were involved, but they want to know whether the third hacker was involved as well.</strong>\n",
    "\n",
    "One last key piece of information you’ve been given by the forensic engineers is that: <blockquote><strong>The Hackers trade off attacks equally!</strong></blockquote> <p style=\"color:red\">For example, imagine there were 100 attack instances. If 2 hackers were involved, then each hacker would have about 50 attacks. but in the case of 3 hackers, each would have only 33 attacks.</p>\n",
    "\n",
    "To help you solve this problem, RhinoTech has provided you with a CSV file (Download from Moodle) that contains statistics about the attacks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Loading dataset  <a class=\"anchor\" name=\"loading\"></a>\n",
    "First, let’s import the necessary Spark libraries we will use for data analysis, preparation, and clustering. Use <code>inferSchema=True</code> and display the schema of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    " \n",
    "# Load csv file using spark session\n",
    "data = spark.read.csv('hack_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Information on Dataset:\n",
    ">Our dataset contains 334 attack instances, with the following information for each one:\n",
    ">    - **Session_Connection_Time** - How long the session lasted in minutes?\n",
    ">    - **Bytes Transferred** - Megabytes transferred during session\n",
    ">    - **Kali_Trace_Used** - Whether the hacker was using Kali Linux\n",
    ">    - **Servers_Corrupted** - Number of server corrupted during the attack\n",
    ">    - **Pages_Corrupted** - Number of pages illegally accessed\n",
    ">    - **Location** - Location attack came from\n",
    ">    - **WPM_Typing_Speed** - Estimated typing speed based on session logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Data preparation <a class=\"anchor\" name=\"preparation\"></a>\n",
    "`MLlib` library in Spark only accepts dataframes that have one column for clustering. This column should contain all the features in the form of vectors, with each vector corresponding to the features in that particular row.\n",
    "\n",
    "<strong style=\"color:red\">IMPORTANT: </strong>The `Location` column will be useless to consider because the hackers probably used VPNs to hide their real locations during the attacks. Therefore, we do not have to include that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used', 'Servers_Corrupted', 'Pages_Corrupted', 'WPM_Typing_Speed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Feature Engineering  <a class=\"anchor\" name=\"fe\"></a>\n",
    "#### Assemble using VectorAssembler\n",
    "We can assemble our attributes into one column using Spark’s **VectorAssembler**. When creating a **VectorAssembler** object, we must specify the input columns and the output column.\n",
    "\n",
    "The input columns are a list of columns that we want to assemble, and the output column is just a name for the column created by the assembler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "assembled_data = assembler.transform(data)\n",
    "assembled_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling : Standard Scaler\n",
    "Next, we need to standardise our data. To accomplish this, Spark has its own **StandardScaler** which takes in two arguments — the name of the input column and the name of the output (scaled) column. <a href=\"https://spark.apache.org/docs/3.0.0/ml-features.html#standardscaler\" target=\"_BLANK\">[REF]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(assembled_data)\n",
    "scaled_data = scaler_model.transform(assembled_data)\n",
    "scaled_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Clustering using K-Means <a class=\"anchor\" name=\"clustering\"></a>\n",
    "To tackle the question of whether there were two hackers or three, we can create two k-means models. One model will be initialized with **two** clusters (k = 2), and the other will be initialized with **three** clusters (k = 3). We will also specify the column we want to pass into the model for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(featuresCol='scaledFeatures', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = k_means.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was the third hacker involved?\n",
    "\n",
    "Finally, it’s time to find out how many hackers were involved with the attacks. Using `.transform()` on the clustering model will transform our dataset so that a new attribute called `predictions` will be created. This new column will contain integers that indicate the cluster to which each attack instance has been classified.\n",
    "Let’s take a look at how many instances are grouped into each cluster in the case of three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(scaled_data)\n",
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-2\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">2. Lab Task: </strong> \n",
    "    Based on the above count you see for each group of attach, what do you think? Does it look like there were 3 hackers involved? Discuss this with your tutor?\n",
    "    </div>\n",
    "\n",
    "<strong>SOLUTION:</strong> No, because the number of instances isn’t similar between the three clusters. Since this goes against our background information that the hackers trade off attacks, it seems unlikely that three hackers were involved.\n",
    "Next, let’s take a look at the instance classifications in the case of two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score <a class=\"anchor\" name=\"ss\"></a>\n",
    "Silhouette Score represents the separation distance between the resulting clusters. Higher the silhouette score, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-3\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">3. Lab Task: </strong> \n",
    "    Recreate the above steps using the <strong>Pipeline API</strong>. In this case use <strong style=\"color:red\">K = 2</strong> in the K-Means Estimator. Also calculate the Silhouette Score for the pipeline model. \n",
    "   \n",
    " <strong>HINT:</strong> You can use <code>model.stages[-1]</code> to access the model from the pipeline.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Load/Prepare the data\n",
    "In this case you can use the same dataset from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the vector Assembler Here\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Standard Scaler here\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : K-Means (k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the K-Means estimator with number of clusters =2\n",
    "k_means = KMeans(featuresCol='scaledFeatures', k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Pipeline\n",
    "Plug all the steps above into a pipeline and generate the prediction count like before and analyze the number of instances in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline(stages=[assembler,scaler,k_means])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(data)\n",
    "predictions = pipelineModel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = pipelineModel.stages[-1].clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-4\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">4. Lab Task: </strong> \n",
    "    Discuss your observations about the following with your tutor:\n",
    "    <ol>\n",
    "        <li>What is the difference is Silhoutte Score with K=3 vs K=2? What do you think is the optimal number of clusters in this case?\n",
    "        <li>Based on the number of instances in each cluster, does it look like there were 3 hackers involved in the attack?</li>\n",
    "    </ol>\n",
    "    </div>\n",
    "    \n",
    "<strong>SOLUTION:</strong> \n",
    "With K=2, the Silhoutte score is a lot better, making 2 as the optimal number of clusters.\n",
    "Both clusters here have exactly the same number of instances assigned to them, and this perfectly aligns with the idea of hackers trading off attacks.\n",
    "Therefore, it is highly likely that only two hackers were involved with the attacks at RhinoTech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal number of clusters based on the Silhouette Score <a class=\"anchor\" name=\"optimal\"></a>\n",
    "We already know, better Silhouette Score signifes a better separation of clusters. To find the optimal number of clusters, we can simply calculate the score for different cluster size and decide the optimal number of clusters based on the maximum score.\n",
    "\n",
    "In the code below, we have calculated the Silhouette score for 2-10 cluster size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are taking the data transformed by the StandardScaler\n",
    "silhouette_arr=[]\n",
    "for k in range(2,10):\n",
    "    k_means= KMeans(featuresCol='scaledFeatures', k=k)\n",
    "    model = k_means.fit(scaled_data)\n",
    "    predictions = model.transform(scaled_data)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_arr.append(silhouette)\n",
    "    print('No of clusters:',k,'Silhouette Score:',silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,10),silhouette_arr)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Silhouette Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
