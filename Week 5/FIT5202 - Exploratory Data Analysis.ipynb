{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcWYqAxbFIee"
   },
   "source": [
    "# FIT5202 Data processing for Big data\n",
    "\n",
    "##  Activity: Exploratory Data Analysis with PySpark\n",
    "\n",
    "In this lab, you will apply the concepts learned to perform a complete EDA on a real-world dataset using Apache PySpark. You will practice loading data, cleaning it, performing univariate and bivariate analysis, and generating visualizations to derive actionable insights.\n",
    "\n",
    "### Dataset:\n",
    "The \"Bank Marketing\" dataset from the UCI Machine Learning Repository. This dataset contains information from a direct marketing campaign of a Portuguese bank.\n",
    "\n",
    "You shall download the dataset from Moodle or visit the original site:\n",
    "https://archive.ics.uci.edu/dataset/222/bank+marketing\n",
    "\n",
    "### Business Context:\n",
    "The bank wants to understand which factors are most influential in a client's decision to subscribe to a term deposit. Our goal is to explore the data to generate hypotheses that could help the bank improve its marketing strategy.\n",
    "\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "### Reference\n",
    "### <div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"> You can refer to https://samplecode.link, learn and try different types of visualisation/plots.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [SparkContext and SparkSession](#one)\n",
    "* [Import Plot Libraries](#import)\n",
    "* [Start: Load Data](#start)        \n",
    "* [Process: Data Cleaning and Preprocessing](#process1)    \n",
    "* [Process: Initial Data Profiling](#process2)    \n",
    "* [Univariate Analysis](#univariate)\n",
    "* [Bivariate/Multivariate Analysis](#multivariate)\n",
    "* [Process: Visualisation & Exploration](#process3)\n",
    "* [Lab Task: Further Process](#process4)\n",
    "* [Findings: Discuss with your peer students](#findings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iChz1a-tk7aP"
   },
   "source": [
    "<a class=\"anchor\" name=\"one\"></a>\n",
    "## Import Spark classes and create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsQiS58Ak7aQ"
   },
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Exploratory Data Analysis\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "783QFsKyk7aV"
   },
   "source": [
    "<a class=\"anchor\" name=\"import\"></a>\n",
    "## Import Plot Libraries\n",
    "\n",
    "Now we import plot libraries, we will use matplotlib and seaborn for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PM_c05Ck7aW",
    "outputId": "601ea949-37f3-45a8-f64f-f080af2a90da"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# this line is needed for the inline display of graphs in Jupyter Notebook\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fytB-sHek7ad"
   },
   "source": [
    "## Start: <a class=\"anchor\" name=\"start\"></a>Data Ingestion and Initial Inspection\n",
    "The first step is to load our data from the CSV file into a PySpark DataFrame. \n",
    "\n",
    "We will then perform some initial checks to ensure the data has been loaded correctly and to get a first look at its structure.\n",
    "\n",
    "Note that this particular dataset uses a semicolon (;) as a separator, so we must specify that when reading the file.\n",
    "\n",
    "### Metadata\n",
    "Metadata is essentially \"data about data\". It gives you information about the dataset.\n",
    "\n",
    "| Variable Name | Role\t| Type |Demographic | Description |\tMissing Values |\n",
    "|----|----|----|----|:----|----|\n",
    "| age | Feature | Integer | Age |  | \tno | \n",
    "| job\t | Feature | \tCategorical | \tOccupation | \ttype of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\t | \tno | \n",
    " | marital | \tFeature | \tCategorical | \tMarital Status | (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\t | \tno | \n",
    " | education | \tFeature | \tCategorical | \tEducation Level  | \t(categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\t | \tno | \n",
    " | default | \tFeature | \tBinary\t | |  \thas credit in default?\t | \tno | \n",
    " | balance | \tFeature | \tInteger\t | \t | average yearly balance\teuros | \tno | \n",
    " | housing | \tFeature\t | Binary\t | \t | has housing loan?\t\t | no | \n",
    " | loan | \tFeature | \tBinary\t | \t | has personal loan?\t | \tno | \n",
    " | contact | Feature\t | Categorical\t\t |  | contact communication type (categorical: 'cellular','telephone')\t | \tyes | \n",
    " | day_of_week\t | Feature | \tDate | \t | \tlast contact day of the week | \t\tno | \n",
    " | month\t | Feature | \tDate\t | \t | last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\t | \tno | \n",
    " | duration\t | Feature | \tInteger\t |  | \tlast contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\t | \tno | \n",
    " | campaign\t | Feature\t | Integer | \t | \tnumber of contacts performed during this campaign and for this client (numeric, includes last contact)\t | \tno | \n",
    " | pdays | \tFeature  | \tInteger\t | \t | number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\t | \tyes | \n",
    " | previous\t | Feature | \tInteger\t |  | \tnumber of contacts performed before this campaign and for this client\t | \tno | \n",
    " | poutcome\t | Feature\t | Categorical | \t | \toutcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\t | \tyes | \n",
    " | y | \tTarget | \tBinary\t |  | \thas the client subscribed a term deposit?\t | \tno | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process: Data Cleaning and Preprocessing <a class=\"anchor\" name=\"process1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# 'header=True' tells Spark to use the first row as the column names.\n",
    "# 'inferSchema=True' tells Spark to go through the data and determine the data type for each column automatically.\n",
    "# 'sep=';'' specifies the delimiter used in the file.\n",
    "file_path = \"data/bank-full.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema to understand the column names and their data types.\n",
    "# This is the PySpark equivalent of pandas'.info() method.\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong>Question</strong>: Look at the schema above. Do the inferred data types seem correct for each column? For example, is age an integer? Is balance an integer?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 10 rows of the DataFrame to get a feel for the data.\n",
    "# This is the PySpark equivalent of pandas'.head() method.\n",
    "print(\"Top 10 rows of the DataFrame:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can optionally convert it to Pandas dataframe for a prettier output.\n",
    "df.pandas_api().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of rows in the DataFrame.\n",
    "#.count() is an \"action\" that triggers a Spark job to scan the entire dataset.\n",
    "total_rows = df.count()\n",
    "print(f\"The dataset contains {total_rows} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning \n",
    "Data cleaning is a critical step. We will check for missing values and then transform our target variable y into a more analysis-friendly numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A robust way to check for null values in each column of a PySpark DataFrame.\n",
    "# We use a list comprehension to apply the check to all columns.\n",
    "print(\"Null value counts per column:\")\n",
    "df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# We can see this dataset is pre-cleaned already; however, in reallife, you often need to deal with dirty dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">The output shows that this dataset is very clean with no missing values. In a real-world project, you would now apply techniques like df.na.drop() or df.na.fill().  </div>  \n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">Next, we will convert the target variable y from string ('yes'/'no') to integer (1/0). This makes it easy to calculate averages, which will directly correspond to the subscription rate. We'll create a new column called subscribed. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the withColumn() transformation to add a new column.\n",
    "# The F.when() function is a SQL-like CASE WHEN statement.\n",
    "# If the value in column 'y' is 'yes', the new 'subscribed' column gets a 1, otherwise it gets a 0.\n",
    "df = df.withColumn('subscribed', F.when(F.col('y') == 'yes', 1).otherwise(0))\n",
    "\n",
    "# Show the result to verify the new column\n",
    "df.select('y', 'subscribed').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process: Initial Data Profiling <a class=\"anchor\" name=\"process2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the pandas describe function to have a quick view of data profiling.\n",
    "df.pandas_api().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process: Univariate Analysis <a class=\"anchor\" name=\"univariate\"></a>\n",
    "Now we begin our analysis by examining individual variables. For numerical columns, we'll look at summary statistics. For categorical columns, we'll look at frequency distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use.describe() to get summary statistics for the numerical columns.\n",
    "# This includes count, mean, standard deviation, min, and max.\n",
    "print(\"Summary statistics for numerical columns:\")\n",
    "df.describe(['age', 'balance', 'duration', 'campaign']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What can you infer from the summary statistics?  \n",
    "What is the average age of a client?  \n",
    "Look at the balance column. The mean is 1362, but the standard deviation is very high (3044). What might this suggest about the distribution of client balances?  \n",
    "The max duration is 4918 seconds (over 80 minutes). Does this seem plausible for a marketing call? What might this represent?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze a categorical variable. We will use the \"aggregate in Spark, plot in Pandas\" pattern. We'll find the frequency of each job type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Perform the aggregation in PySpark\n",
    "# Group by the 'job' column and count the occurrences of each.\n",
    "# Order by the count in descending order for a cleaner plot.\n",
    "job_counts_df = df.groupBy('job').count().orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert the small, aggregated result to a Pandas DataFrame\n",
    "job_counts_pd = job_counts_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create the plot using Matplotlib/Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=job_counts_pd, x='job', y='count', hue='job')\n",
    "plt.title('Frequency of Client Job Types')\n",
    "plt.xlabel('Job Type')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Exercise: Now, create similar bar charts for the marital and education columns. What are the most common categories in each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for 'marital' status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for 'education' level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process: Bivariate/Multivariate Analysis <a class=\"anchor\" name=\"multivariate\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">Here, we explore relationships between variables, focusing on how they relate to our target: subscribed.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: How does age affect subscription status?\n",
    "We can visualize this by plotting the distribution of age for those who subscribed versus those who did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter the data in PySpark\n",
    "subscribed_ages_df = df.filter(F.col('subscribed') == 1).select('age')\n",
    "not_subscribed_ages_df = df.filter(F.col('subscribed') == 0).select('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert the results to Pandas\n",
    "subscribed_ages_pd = subscribed_ages_df.toPandas()\n",
    "not_subscribed_ages_pd = not_subscribed_ages_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Plot the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(data=subscribed_ages_pd['age'], label='Subscribed', color='green', fill=True)\n",
    "sns.kdeplot(data=not_subscribed_ages_pd['age'], label='Not Subscribed', color='red', fill=True)\n",
    "plt.title('Age Distribution by Subscription Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">\n",
    "    <strong>Interpretation:</strong> The plot shows that while the overall distributions are similar, the green \"Subscribed\" curve has noticeable bumps at older ages (around 60+) compared to the red \"Not Subscribed\" curve. This suggests that older clients might be more likely to subscribe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Which job types have the highest subscription rates?\n",
    "Since our subscribed column is 0 or 1, the average of this column for a given group is equal to the rate of 1s (i.e., the subscription rate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate in PySpark to calculate the average of 'subscribed' for each job.\n",
    "job_subscription_rate_df = df.groupBy('job').agg(F.avg('subscribed').alias('subscription_rate')).orderBy(F.desc('subscription_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert to Pandas\n",
    "job_subscription_rate_pd = job_subscription_rate_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=job_subscription_rate_pd, x='job', y='subscription_rate', hue='job')\n",
    "plt.title('Subscription Rate by Job Type')\n",
    "plt.xlabel('Job Type')\n",
    "plt.ylabel('Subscription Rate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">Question: Which job types have the highest and lowest subscription rates? Does this result surprise you, given what we saw in the univariate analysis? (Hint: blue-collar was the most common job type, but has one of the lowest subscription rates).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process: Visualisation & Exploration <a class=\"anchor\" name=\"process3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Is there a relationship between numerical variables?\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">A correlation matrix is an excellent tool for this. We will select a few key numerical columns, bring them to Pandas (as this calculation is feasible on a sample or subset), and create a heatmap.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select numerical columns in PySpark\n",
    "numerical_cols = ['age', 'balance', 'duration', 'campaign', 'previous']\n",
    "numerical_df = df.select(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert to Pandas (as this is a small number of columns, we can do this directly for correlation)\n",
    "corr_pd = numerical_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the correlation matrix and plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = corr_pd.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Key Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\">Interpretation:  \n",
    "The heatmap shows the correlation coefficients between pairs of variables.  \n",
    "Notice the relatively strong positive correlation between duration and our target (not shown here but would be if we included subscribed).  \n",
    "This indicates that longer call durations are associated with a higher likelihood of subscription.  \n",
    "This makes intuitive sense, but it's important to consider that the call is long because the customer is interested, not the other way around.  \n",
    "This is known as \"data leakage\" if used as a predictor in a model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task: Further Process <a class=\"anchor\" name=\"process4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"> Go to https://samplecode.link, learn and try other types of visualisation/plots.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset also has bank-additional-full.csv, explore this additional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task: Findings, Discuss Your Findings with Peer Students <a class=\"anchor\" name=\"findings\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most important part of EDA: synthesizing our findings into actionable insights and testable hypotheses.   \n",
    "Discuss your finding with your peer students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations on completing this lab!  \n",
    "You have successfully performed a comprehensive Exploratory Data Analysis on a real-world dataset using Apache PySpark.  \n",
    "You have practiced the entire workflow:  \n",
    "- Loading and inspecting large-scale data.  \n",
    "- Cleaning and preprocessing variables.  \n",
    "- Analyzing individual variables (univariate analysis).  \n",
    "- Exploring relationships between variables (bivariate analysis).  \n",
    "- Mastering the \"aggregate in Spark, plot in Pandas\" pattern for visualization.  \n",
    "- Translating analytical findings into business-relevant insights and hypotheses.  \n",
    "- The insights generated here would be the direct input for the next phase of a data science project, which could involve building a machine learning model to predict which customers are most likely to subscribe. Your EDA has helped identify which features are likely to be important predictors (job, age, duration) and has provided a deep, foundational understanding of the data.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yVWYWwzMFIfR",
    "K2QtBnKgFIfa",
    "kL88Q46yFIfh",
    "48_7UVktFIgD",
    "dtN67ydpFIgF",
    "cSs0qd02FIgI"
   ],
   "name": "FIT5202 - Parallel Aggregation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
